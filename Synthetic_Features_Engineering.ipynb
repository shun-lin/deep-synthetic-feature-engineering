{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Synthetic_Features_Engineering.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":["xYYA6GfFN1mb","ydsc-4NUN6jA","Z4nsUEh9OIUV"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"Jbvf2aHA0jNs","colab_type":"text"},"cell_type":"markdown","source":["# Synthetic Features Engineering Notebook on Google Colab (GPU)\n","\n","This notebook is modified to work on Google Colab, which provides free GPU / TPU resources for machine learning tasks.\n","\n","Everyone who has access to this google drive folder should have access to this notebook and should be edit it directly on Google Colab.\n"]},{"metadata":{"id":"1F5WY5hw2BDK","colab_type":"text"},"cell_type":"markdown","source":["## README and Developer Guide\n","\n","We will write and modify code DIRECTLY on **Google Colab**, which is not automatically synced with the version on our github repository. Make sure that you **DOWNLOAD** this notebook after edit and upload it to our **github repository** for a more consistent version control.\n","\n","Make sure you add the Paradigm folder to your drive before runnning the code. To do so right click the folder and select **Add to my drive** from the dropdown menu.\n","\n","Because of the nature of Google Drive, it is very hard to edit the same ipython file on the same time on Colab. If someone is editing the colab file at the same time as you, you will need to **restart** the runtime, so be mindful of that if someone is editing the notebook at the same time as you.\n","\n","We are fairly new to Google Colab and thus feel free to add anything to the README section if you come across anything that is important when coding!\n","\n","This notebook will be geared to use **GPU** to help accelerate the training process. Please make sure that when you run this notebook the backend if using **GPU**. To do so click 'Runtime' then select 'Change runtime type' and make sure that the hardware accelerator is set to GPU. If you are interested in using **TPU** to accelerate the training process even **FASTER**, please create another notebook to run the TPU-Compatible model."]},{"metadata":{"id":"G4cIzHHy39-M","colab_type":"text"},"cell_type":"markdown","source":["## (0) SETUP\n","\n","1.   Grant Google Colab access to Google Drive\n","2.   Import libraries and tools\n","\n","\n","\n","\n","\n"]},{"metadata":{"id":"2CrtVDmg0kIw","colab_type":"code","outputId":"b57dac5f-dc0d-4e11-f010-08e53492b260","executionInfo":{"status":"ok","timestamp":1554922565592,"user_tz":420,"elapsed":21368,"user":{"displayName":"SHUN LIN","photoUrl":"https://lh4.googleusercontent.com/-pkp40ccE7So/AAAAAAAAAAI/AAAAAAAAAU4/Upp1QcV6fHs/s64/photo.jpg","userId":"16137932526864003348"}},"colab":{"base_uri":"https://localhost:8080/","height":122}},"cell_type":"code","source":["# Getting access to the dataset and the Python files on Google Drive.\n","# You will have to give permission and sign in to your Google account.\n","\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n","root_folder = \"/content/gdrive/My Drive/Paradigm/Paradigm (Spr 19) - Team 2/Colab-code/\""],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n"],"name":"stdout"}]},{"metadata":{"id":"U8hKmXJyZSwN","colab_type":"code","outputId":"d0230606-611c-450b-b993-d422c34c7944","executionInfo":{"status":"ok","timestamp":1554922571556,"user_tz":420,"elapsed":27266,"user":{"displayName":"SHUN LIN","photoUrl":"https://lh4.googleusercontent.com/-pkp40ccE7So/AAAAAAAAAAI/AAAAAAAAAU4/Upp1QcV6fHs/s64/photo.jpg","userId":"16137932526864003348"}},"colab":{"base_uri":"https://localhost:8080/","height":190}},"cell_type":"code","source":["# install segtok\n","!pip install segtok"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Collecting segtok\n","  Downloading https://files.pythonhosted.org/packages/1d/59/6ed78856ab99d2da04084b59e7da797972baa0efecb71546b16d48e49d9b/segtok-1.5.7.tar.gz\n","Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from segtok) (2018.1.10)\n","Building wheels for collected packages: segtok\n","  Building wheel for segtok (setup.py) ... \u001b[?25ldone\n","\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/15/ee/a8/6112173f1386d33eebedb3f73429cfa41a4c3084556bcee254\n","Successfully built segtok\n","Installing collected packages: segtok\n","Successfully installed segtok-1.5.7\n"],"name":"stdout"}]},{"metadata":{"id":"4wgY1xMi43ms","colab_type":"code","colab":{}},"cell_type":"code","source":["# general imports\n","from __future__ import absolute_import, division, print_function\n","\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","import collections\n","import os\n","import string\n","import time\n","from segtok import tokenizer\n","from collections import Counter\n","import json\n","import sys"],"execution_count":0,"outputs":[]},{"metadata":{"id":"KK9eQkWi5Poc","colab_type":"code","outputId":"4f615ba1-8491-4c04-b7f3-534e9bdc96e3","executionInfo":{"status":"ok","timestamp":1554922573967,"user_tz":420,"elapsed":29625,"user":{"displayName":"SHUN LIN","photoUrl":"https://lh4.googleusercontent.com/-pkp40ccE7So/AAAAAAAAAAI/AAAAAAAAAU4/Upp1QcV6fHs/s64/photo.jpg","userId":"16137932526864003348"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["# machine learning libraries imports\n","import keras\n","import sklearn\n","import tensorflow as tf\n","\n","from keras import backend as K\n","from keras import layers\n","from keras.models import Model, Sequential\n","from keras.layers import Dense, Embedding, Input, Lambda, LSTM, Masking, RepeatVector, TimeDistributed\n","from keras.preprocessing.text import Tokenizer\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.model_selection import train_test_split"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"metadata":{"id":"XkaR9Zy2JRa_","colab_type":"text"},"cell_type":"markdown","source":["## (1) EDA\n","\n","\n","1.   Import data \n","2.   Analysize data\n","3.   Clean data\n","4.  Graph relationships\n","\n","**NOTE**: The dataset we have is not very big (around 7000 cyptocurrency related articles). If time permitted we can scrap older news article from [bitCoinTalk Thread](https://bitcointalk.org/index.php?board=77.0)\n"]},{"metadata":{"id":"RdejphAAVlW4","colab_type":"text"},"cell_type":"markdown","source":["### (1.1) Import data from the data folder"]},{"metadata":{"id":"0QYxDh2l5T5m","colab_type":"code","outputId":"087840c5-a28c-40d6-8498-134e32fc6396","executionInfo":{"status":"ok","timestamp":1554922575112,"user_tz":420,"elapsed":30671,"user":{"displayName":"SHUN LIN","photoUrl":"https://lh4.googleusercontent.com/-pkp40ccE7So/AAAAAAAAAAI/AAAAAAAAAU4/Upp1QcV6fHs/s64/photo.jpg","userId":"16137932526864003348"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"cell_type":"code","source":["# get a list of data we have\n","\n","data_folder = root_folder + 'data/'\n","print(\"We have gathered the following datasets\")\n","print(os.listdir(data_folder))\n"],"execution_count":5,"outputs":[{"output_type":"stream","text":["We have gathered the following datasets\n","['news_score.csv', '1119_cleaned_author_articles.csv', 'data_onehot.csv', 'rawData_test1009.csv']\n"],"name":"stdout"}]},{"metadata":{"id":"_mqvydQbLTEZ","colab_type":"code","colab":{}},"cell_type":"code","source":["# importing data from csv to dataframe\n","news_score_df = pd.read_csv(data_folder + 'news_score.csv')\n","raw_data_df = pd.read_csv(data_folder + 'rawData_test1009.csv')\n","cleaned_author_df = pd.read_csv(data_folder + '1119_cleaned_author_articles.csv')\n","onehot_df = pd.read_csv(data_folder + 'data_onehot.csv')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"cmRmIt1YNwYj","colab_type":"text"},"cell_type":"markdown","source":["### (1.2) EDA on News Score Dataframe"]},{"metadata":{"id":"pGxsSKlXNMRr","colab_type":"code","outputId":"d6504189-7c17-41b1-9668-403575510186","executionInfo":{"status":"ok","timestamp":1554655011967,"user_tz":420,"elapsed":761,"user":{"displayName":"SHUN LIN","photoUrl":"https://lh4.googleusercontent.com/-pkp40ccE7So/AAAAAAAAAAI/AAAAAAAAAU4/Upp1QcV6fHs/s64/photo.jpg","userId":"16137932526864003348"}},"colab":{"base_uri":"https://localhost:8080/","height":576}},"cell_type":"code","source":["print(\"There are \" + str(len(news_score_df)) + \" entries in this dataframe.\")\n","sum_of_nans = sum(len(news_score_df) - news_score_df.count())\n","print(\"There are \" + str(sum_of_nans) + \" Nan values in the dataframe.\")\n","\n","# take a look at the news scores dataframe\n","news_score_df.head()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["There are 7101 entries in this dataframe.\n","There are 34 Nan values in the dataframe.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>Unnamed: 0.1</th>\n","      <th>author</th>\n","      <th>contents</th>\n","      <th>description</th>\n","      <th>publisher</th>\n","      <th>source_url</th>\n","      <th>title</th>\n","      <th>date</th>\n","      <th>time</th>\n","      <th>...</th>\n","      <th>Volume_(Currency)</th>\n","      <th>Weighted_Price</th>\n","      <th>Average</th>\n","      <th>Volatility</th>\n","      <th>SD</th>\n","      <th>publisherLabel</th>\n","      <th>Date_x</th>\n","      <th>Date_y</th>\n","      <th>Mark</th>\n","      <th>tfidf</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>3270</td>\n","      <td>Bitcoinist.net</td>\n","      <td>real time prices vires numeris bitcoin ethereu...</td>\n","      <td>israel finance ministry bank israel considerin...</td>\n","      <td>Bitcoinist.com</td>\n","      <td>http://bitcoinist.com/kosher-crypto-bitcoen-se...</td>\n","      <td>Kosher Crypto BitCoen Is Setting a Course for ...</td>\n","      <td>2018-02-02</td>\n","      <td>00:00:08</td>\n","      <td>...</td>\n","      <td>300510.207066</td>\n","      <td>8547.594042</td>\n","      <td>12603.493539</td>\n","      <td>433909.334353</td>\n","      <td>2087.647258</td>\n","      <td>96</td>\n","      <td>2018-02-02</td>\n","      <td>2018-02-02</td>\n","      <td>1.0</td>\n","      <td>0.31574</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>3271</td>\n","      <td>Michelle Fox</td>\n","      <td>var postloadfunctions var foresee enabled var ...</td>\n","      <td>bitcoin may still drop rally back year early b...</td>\n","      <td>CNBC</td>\n","      <td>https://www.cnbc.com/2018/02/01/bitcoin-near-b...</td>\n","      <td>Bitcoin near bottom, will rally to $20,000 thi...</td>\n","      <td>2018-02-02</td>\n","      <td>00:02:00</td>\n","      <td>...</td>\n","      <td>300510.207066</td>\n","      <td>8547.594042</td>\n","      <td>12603.493539</td>\n","      <td>433909.334353</td>\n","      <td>2087.647258</td>\n","      <td>146</td>\n","      <td>2018-02-02</td>\n","      <td>2018-02-02</td>\n","      <td>1.0</td>\n","      <td>0.49906</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>3272</td>\n","      <td>Scott Scanlon</td>\n","      <td>at core cryptocurrency networks miners people ...</td>\n","      <td>core cryptocurrency networks miners people use...</td>\n","      <td>Youbrandinc.com</td>\n","      <td>https://www.youbrandinc.com/crytocurrency/brai...</td>\n","      <td>Brain Genius Submerges His Bitcoin Mining Rig ...</td>\n","      <td>2018-02-02</td>\n","      <td>00:03:08</td>\n","      <td>...</td>\n","      <td>300510.207066</td>\n","      <td>8547.594042</td>\n","      <td>12603.493539</td>\n","      <td>433909.334353</td>\n","      <td>2087.647258</td>\n","      <td>1181</td>\n","      <td>2018-02-02</td>\n","      <td>2018-02-02</td>\n","      <td>1.0</td>\n","      <td>0.31824</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>3273</td>\n","      <td>Bruce Kleinman</td>\n","      <td>demons digital gold part if already done pleas...</td>\n","      <td>demons digital gold part</td>\n","      <td>Hackernoon.com</td>\n","      <td>https://hackernoon.com/remediation-wherefore-a...</td>\n","      <td>Remediation, wherefore art thou?</td>\n","      <td>2018-02-02</td>\n","      <td>00:18:34</td>\n","      <td>...</td>\n","      <td>300510.207066</td>\n","      <td>8547.594042</td>\n","      <td>12603.493539</td>\n","      <td>433909.334353</td>\n","      <td>2087.647258</td>\n","      <td>452</td>\n","      <td>2018-02-02</td>\n","      <td>2018-02-02</td>\n","      <td>1.0</td>\n","      <td>0.97609</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>3274</td>\n","      <td>Jason Murphy</td>\n","      <td>email password remember me feb while systems u...</td>\n","      <td>systems underpinning bitcoin truly revolutiona...</td>\n","      <td>Crikey.com.au</td>\n","      <td>https://www.crikey.com.au/2018/02/02/cryptotra...</td>\n","      <td>Cryptotragedy: what if bitcoin’s greatest stre...</td>\n","      <td>2018-02-02</td>\n","      <td>00:25:09</td>\n","      <td>...</td>\n","      <td>300510.207066</td>\n","      <td>8547.594042</td>\n","      <td>12603.493539</td>\n","      <td>433909.334353</td>\n","      <td>2087.647258</td>\n","      <td>220</td>\n","      <td>2018-02-02</td>\n","      <td>2018-02-02</td>\n","      <td>1.0</td>\n","      <td>0.33146</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 25 columns</p>\n","</div>"],"text/plain":["   Unnamed: 0  Unnamed: 0.1          author  \\\n","0           0          3270  Bitcoinist.net   \n","1           1          3271    Michelle Fox   \n","2           2          3272   Scott Scanlon   \n","3           3          3273  Bruce Kleinman   \n","4           4          3274    Jason Murphy   \n","\n","                                            contents  \\\n","0  real time prices vires numeris bitcoin ethereu...   \n","1  var postloadfunctions var foresee enabled var ...   \n","2  at core cryptocurrency networks miners people ...   \n","3  demons digital gold part if already done pleas...   \n","4  email password remember me feb while systems u...   \n","\n","                                         description        publisher  \\\n","0  israel finance ministry bank israel considerin...   Bitcoinist.com   \n","1  bitcoin may still drop rally back year early b...             CNBC   \n","2  core cryptocurrency networks miners people use...  Youbrandinc.com   \n","3                           demons digital gold part   Hackernoon.com   \n","4  systems underpinning bitcoin truly revolutiona...    Crikey.com.au   \n","\n","                                          source_url  \\\n","0  http://bitcoinist.com/kosher-crypto-bitcoen-se...   \n","1  https://www.cnbc.com/2018/02/01/bitcoin-near-b...   \n","2  https://www.youbrandinc.com/crytocurrency/brai...   \n","3  https://hackernoon.com/remediation-wherefore-a...   \n","4  https://www.crikey.com.au/2018/02/02/cryptotra...   \n","\n","                                               title        date      time  \\\n","0  Kosher Crypto BitCoen Is Setting a Course for ...  2018-02-02  00:00:08   \n","1  Bitcoin near bottom, will rally to $20,000 thi...  2018-02-02  00:02:00   \n","2  Brain Genius Submerges His Bitcoin Mining Rig ...  2018-02-02  00:03:08   \n","3                   Remediation, wherefore art thou?  2018-02-02  00:18:34   \n","4  Cryptotragedy: what if bitcoin’s greatest stre...  2018-02-02  00:25:09   \n","\n","    ...     Volume_(Currency)  Weighted_Price       Average     Volatility  \\\n","0   ...         300510.207066     8547.594042  12603.493539  433909.334353   \n","1   ...         300510.207066     8547.594042  12603.493539  433909.334353   \n","2   ...         300510.207066     8547.594042  12603.493539  433909.334353   \n","3   ...         300510.207066     8547.594042  12603.493539  433909.334353   \n","4   ...         300510.207066     8547.594042  12603.493539  433909.334353   \n","\n","            SD  publisherLabel      Date_x      Date_y  Mark    tfidf  \n","0  2087.647258              96  2018-02-02  2018-02-02   1.0  0.31574  \n","1  2087.647258             146  2018-02-02  2018-02-02   1.0  0.49906  \n","2  2087.647258            1181  2018-02-02  2018-02-02   1.0  0.31824  \n","3  2087.647258             452  2018-02-02  2018-02-02   1.0  0.97609  \n","4  2087.647258             220  2018-02-02  2018-02-02   1.0  0.33146  \n","\n","[5 rows x 25 columns]"]},"metadata":{"tags":[]},"execution_count":29}]},{"metadata":{"id":"d13QewRKQrz4","colab_type":"code","outputId":"2806a48b-cacf-4343-e9fb-8df6690b74eb","executionInfo":{"status":"ok","timestamp":1554655012369,"user_tz":420,"elapsed":602,"user":{"displayName":"SHUN LIN","photoUrl":"https://lh4.googleusercontent.com/-pkp40ccE7So/AAAAAAAAAAI/AAAAAAAAAU4/Upp1QcV6fHs/s64/photo.jpg","userId":"16137932526864003348"}},"colab":{"base_uri":"https://localhost:8080/","height":459}},"cell_type":"code","source":["# find out which column has NAN values\n","news_score_df.isna().sum()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Unnamed: 0            0\n","Unnamed: 0.1          0\n","author               24\n","contents              9\n","description           1\n","publisher             0\n","source_url            0\n","title                 0\n","date                  0\n","time                  0\n","Open                  0\n","High                  0\n","Low                   0\n","Close                 0\n","Volume_(BTC)          0\n","Volume_(Currency)     0\n","Weighted_Price        0\n","Average               0\n","Volatility            0\n","SD                    0\n","publisherLabel        0\n","Date_x                0\n","Date_y                0\n","Mark                  0\n","tfidf                 0\n","dtype: int64"]},"metadata":{"tags":[]},"execution_count":30}]},{"metadata":{"id":"7UukGv2EORHW","colab_type":"code","outputId":"5691153f-b998-495e-88fe-01328838d176","executionInfo":{"status":"ok","timestamp":1554655016365,"user_tz":420,"elapsed":703,"user":{"displayName":"SHUN LIN","photoUrl":"https://lh4.googleusercontent.com/-pkp40ccE7So/AAAAAAAAAAI/AAAAAAAAAU4/Upp1QcV6fHs/s64/photo.jpg","userId":"16137932526864003348"}},"colab":{"base_uri":"https://localhost:8080/","height":119}},"cell_type":"code","source":["# get the list of news score dataframe columns\n","print(news_score_df.columns)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Index(['Unnamed: 0', 'Unnamed: 0.1', 'author', 'contents', 'description',\n","       'publisher', 'source_url', 'title', 'date', 'time', 'Open', 'High',\n","       'Low', 'Close', 'Volume_(BTC)', 'Volume_(Currency)', 'Weighted_Price',\n","       'Average', 'Volatility', 'SD', 'publisherLabel', 'Date_x', 'Date_y',\n","       'Mark', 'tfidf'],\n","      dtype='object')\n"],"name":"stdout"}]},{"metadata":{"id":"dfR5QNEmOlnP","colab_type":"code","outputId":"7c78dace-3684-4184-e3eb-555387622cf8","executionInfo":{"status":"ok","timestamp":1554655016954,"user_tz":420,"elapsed":807,"user":{"displayName":"SHUN LIN","photoUrl":"https://lh4.googleusercontent.com/-pkp40ccE7So/AAAAAAAAAAI/AAAAAAAAAU4/Upp1QcV6fHs/s64/photo.jpg","userId":"16137932526864003348"}},"colab":{"base_uri":"https://localhost:8080/","height":334}},"cell_type":"code","source":["# describe the dataframe\n","news_score_df.describe()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>Unnamed: 0.1</th>\n","      <th>Open</th>\n","      <th>High</th>\n","      <th>Low</th>\n","      <th>Close</th>\n","      <th>Volume_(BTC)</th>\n","      <th>Volume_(Currency)</th>\n","      <th>Weighted_Price</th>\n","      <th>Average</th>\n","      <th>Volatility</th>\n","      <th>SD</th>\n","      <th>publisherLabel</th>\n","      <th>Mark</th>\n","      <th>tfidf</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>count</th>\n","      <td>7101.000000</td>\n","      <td>7101.000000</td>\n","      <td>7101.000000</td>\n","      <td>7101.000000</td>\n","      <td>7101.000000</td>\n","      <td>7101.000000</td>\n","      <td>7101.000000</td>\n","      <td>7101.000000</td>\n","      <td>7101.000000</td>\n","      <td>7101.000000</td>\n","      <td>7101.000000</td>\n","      <td>7101.000000</td>\n","      <td>7101.000000</td>\n","      <td>7101.0</td>\n","      <td>7101.000000</td>\n","    </tr>\n","    <tr>\n","      <th>mean</th>\n","      <td>3550.000000</td>\n","      <td>10965.699901</td>\n","      <td>8181.754966</td>\n","      <td>8190.224874</td>\n","      <td>8172.818887</td>\n","      <td>8181.687325</td>\n","      <td>25.769918</td>\n","      <td>199844.618770</td>\n","      <td>8181.520405</td>\n","      <td>10700.982828</td>\n","      <td>293349.820050</td>\n","      <td>1411.379980</td>\n","      <td>614.334178</td>\n","      <td>1.0</td>\n","      <td>0.407376</td>\n","    </tr>\n","    <tr>\n","      <th>std</th>\n","      <td>2050.026463</td>\n","      <td>5089.337475</td>\n","      <td>682.738963</td>\n","      <td>678.624219</td>\n","      <td>687.013524</td>\n","      <td>682.631745</td>\n","      <td>16.917757</td>\n","      <td>110424.930092</td>\n","      <td>682.783262</td>\n","      <td>893.982685</td>\n","      <td>116728.369425</td>\n","      <td>561.609629</td>\n","      <td>378.612438</td>\n","      <td>0.0</td>\n","      <td>0.186766</td>\n","    </tr>\n","    <tr>\n","      <th>min</th>\n","      <td>0.000000</td>\n","      <td>3270.000000</td>\n","      <td>6806.927451</td>\n","      <td>6826.417535</td>\n","      <td>6786.712438</td>\n","      <td>6807.273597</td>\n","      <td>9.265880</td>\n","      <td>74305.946821</td>\n","      <td>6806.651598</td>\n","      <td>9786.788157</td>\n","      <td>180619.004534</td>\n","      <td>869.003591</td>\n","      <td>0.000000</td>\n","      <td>1.0</td>\n","      <td>0.157550</td>\n","    </tr>\n","    <tr>\n","      <th>25%</th>\n","      <td>1775.000000</td>\n","      <td>5045.000000</td>\n","      <td>7905.440896</td>\n","      <td>7917.342931</td>\n","      <td>7892.443951</td>\n","      <td>7905.416028</td>\n","      <td>13.935292</td>\n","      <td>121295.129238</td>\n","      <td>7904.972491</td>\n","      <td>10090.346928</td>\n","      <td>192710.683522</td>\n","      <td>927.179708</td>\n","      <td>199.000000</td>\n","      <td>1.0</td>\n","      <td>0.281210</td>\n","    </tr>\n","    <tr>\n","      <th>50%</th>\n","      <td>3550.000000</td>\n","      <td>13503.000000</td>\n","      <td>8310.115639</td>\n","      <td>8313.456646</td>\n","      <td>8306.793611</td>\n","      <td>8310.097187</td>\n","      <td>18.984345</td>\n","      <td>154043.483808</td>\n","      <td>8310.129580</td>\n","      <td>10130.017603</td>\n","      <td>231620.464917</td>\n","      <td>1114.384481</td>\n","      <td>640.000000</td>\n","      <td>1.0</td>\n","      <td>0.329610</td>\n","    </tr>\n","    <tr>\n","      <th>75%</th>\n","      <td>5325.000000</td>\n","      <td>15278.000000</td>\n","      <td>8617.131229</td>\n","      <td>8620.755771</td>\n","      <td>8613.696326</td>\n","      <td>8617.348722</td>\n","      <td>35.615632</td>\n","      <td>297975.901995</td>\n","      <td>8617.210484</td>\n","      <td>11592.976647</td>\n","      <td>437801.267554</td>\n","      <td>2106.372331</td>\n","      <td>967.000000</td>\n","      <td>1.0</td>\n","      <td>0.490910</td>\n","    </tr>\n","    <tr>\n","      <th>max</th>\n","      <td>7100.000000</td>\n","      <td>17053.000000</td>\n","      <td>9435.828417</td>\n","      <td>9441.771257</td>\n","      <td>9429.061993</td>\n","      <td>9435.448514</td>\n","      <td>63.421083</td>\n","      <td>432929.362386</td>\n","      <td>9435.411315</td>\n","      <td>12603.493539</td>\n","      <td>452638.412175</td>\n","      <td>2177.757576</td>\n","      <td>1186.000000</td>\n","      <td>1.0</td>\n","      <td>1.634970</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["        Unnamed: 0  Unnamed: 0.1         Open         High          Low  \\\n","count  7101.000000   7101.000000  7101.000000  7101.000000  7101.000000   \n","mean   3550.000000  10965.699901  8181.754966  8190.224874  8172.818887   \n","std    2050.026463   5089.337475   682.738963   678.624219   687.013524   \n","min       0.000000   3270.000000  6806.927451  6826.417535  6786.712438   \n","25%    1775.000000   5045.000000  7905.440896  7917.342931  7892.443951   \n","50%    3550.000000  13503.000000  8310.115639  8313.456646  8306.793611   \n","75%    5325.000000  15278.000000  8617.131229  8620.755771  8613.696326   \n","max    7100.000000  17053.000000  9435.828417  9441.771257  9429.061993   \n","\n","             Close  Volume_(BTC)  Volume_(Currency)  Weighted_Price  \\\n","count  7101.000000   7101.000000        7101.000000     7101.000000   \n","mean   8181.687325     25.769918      199844.618770     8181.520405   \n","std     682.631745     16.917757      110424.930092      682.783262   \n","min    6807.273597      9.265880       74305.946821     6806.651598   \n","25%    7905.416028     13.935292      121295.129238     7904.972491   \n","50%    8310.097187     18.984345      154043.483808     8310.129580   \n","75%    8617.348722     35.615632      297975.901995     8617.210484   \n","max    9435.448514     63.421083      432929.362386     9435.411315   \n","\n","            Average     Volatility           SD  publisherLabel    Mark  \\\n","count   7101.000000    7101.000000  7101.000000     7101.000000  7101.0   \n","mean   10700.982828  293349.820050  1411.379980      614.334178     1.0   \n","std      893.982685  116728.369425   561.609629      378.612438     0.0   \n","min     9786.788157  180619.004534   869.003591        0.000000     1.0   \n","25%    10090.346928  192710.683522   927.179708      199.000000     1.0   \n","50%    10130.017603  231620.464917  1114.384481      640.000000     1.0   \n","75%    11592.976647  437801.267554  2106.372331      967.000000     1.0   \n","max    12603.493539  452638.412175  2177.757576     1186.000000     1.0   \n","\n","             tfidf  \n","count  7101.000000  \n","mean      0.407376  \n","std       0.186766  \n","min       0.157550  \n","25%       0.281210  \n","50%       0.329610  \n","75%       0.490910  \n","max       1.634970  "]},"metadata":{"tags":[]},"execution_count":32}]},{"metadata":{"id":"xYYA6GfFN1mb","colab_type":"text"},"cell_type":"markdown","source":["### (1.3) EDA on Raw Data Dataframe"]},{"metadata":{"id":"M3M26sfjQ_nT","colab_type":"code","outputId":"d21635d9-b8c3-45d8-89dd-2572df02b6dc","executionInfo":{"status":"ok","timestamp":1554655021326,"user_tz":420,"elapsed":883,"user":{"displayName":"SHUN LIN","photoUrl":"https://lh4.googleusercontent.com/-pkp40ccE7So/AAAAAAAAAAI/AAAAAAAAAU4/Upp1QcV6fHs/s64/photo.jpg","userId":"16137932526864003348"}},"colab":{"base_uri":"https://localhost:8080/","height":323}},"cell_type":"code","source":["print(\"There are \" + str(len(raw_data_df)) + \" entries in this dataframe.\")\n","sum_of_nans = sum(len(raw_data_df) - raw_data_df.count())\n","print(\"There are \" + str(sum_of_nans) + \" Nan values in the dataframe.\")\n","\n","# take a look at the news scores dataframe\n","raw_data_df.head()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["There are 858 entries in this dataframe.\n","There are 48 Nan values in the dataframe.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>author</th>\n","      <th>title</th>\n","      <th>publisher</th>\n","      <th>source_url</th>\n","      <th>timeStamp</th>\n","      <th>content</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Tyler Durden</td>\n","      <td>Internet Censorship Just Took An Unprecedented...</td>\n","      <td>Zerohedge.com</td>\n","      <td>https://www.zerohedge.com/news/2018-10-14/inte...</td>\n","      <td>2018-10-15 00:00:00+00:00</td>\n","      <td>Authored by Cailtin Johnstone via Medium.com, ...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Osato Avan-Nomayo</td>\n","      <td>Bitcoin Mining: Three Reasons Why Energy Consu...</td>\n","      <td>Bitcoinist.com</td>\n","      <td>https://bitcoinist.com/bitcoin-mining-three-re...</td>\n","      <td>2018-10-14 23:00:47+00:00</td>\n","      <td>When they aren’t bashing Bitcoin as a bubble, ...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>James Mickleboro</td>\n","      <td>Bitcoin, Ethereum, and Ripple mixed after Dr D...</td>\n","      <td>Fool.com.au</td>\n","      <td>https://www.fool.com.au/2018/10/15/bitcoin-eth...</td>\n","      <td>2018-10-14 21:30:51+00:00</td>\n","      <td>It has been a reasonably subdued weekend of tr...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Yashu Gola</td>\n","      <td>Report: Crypto Funds Make up 20% of Hedge Fund...</td>\n","      <td>Crypto Coins News</td>\n","      <td>https://www.ccn.com/report-crypto-funds-makes-...</td>\n","      <td>2018-10-14 21:03:13+00:00</td>\n","      <td>The number of cryptocurrency hedge fund launch...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>besguerra</td>\n","      <td>PH energy firms ready to adopt blockchain tech...</td>\n","      <td>Inquirer.net</td>\n","      <td>https://business.inquirer.net/258968/ph-energy...</td>\n","      <td>2018-10-14 21:02:03+00:00</td>\n","      <td>The blockchain business is expected to explode...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["              author                                              title  \\\n","0       Tyler Durden  Internet Censorship Just Took An Unprecedented...   \n","1  Osato Avan-Nomayo  Bitcoin Mining: Three Reasons Why Energy Consu...   \n","2   James Mickleboro  Bitcoin, Ethereum, and Ripple mixed after Dr D...   \n","3         Yashu Gola  Report: Crypto Funds Make up 20% of Hedge Fund...   \n","4          besguerra  PH energy firms ready to adopt blockchain tech...   \n","\n","           publisher                                         source_url  \\\n","0      Zerohedge.com  https://www.zerohedge.com/news/2018-10-14/inte...   \n","1     Bitcoinist.com  https://bitcoinist.com/bitcoin-mining-three-re...   \n","2        Fool.com.au  https://www.fool.com.au/2018/10/15/bitcoin-eth...   \n","3  Crypto Coins News  https://www.ccn.com/report-crypto-funds-makes-...   \n","4       Inquirer.net  https://business.inquirer.net/258968/ph-energy...   \n","\n","                   timeStamp  \\\n","0  2018-10-15 00:00:00+00:00   \n","1  2018-10-14 23:00:47+00:00   \n","2  2018-10-14 21:30:51+00:00   \n","3  2018-10-14 21:03:13+00:00   \n","4  2018-10-14 21:02:03+00:00   \n","\n","                                             content  \n","0  Authored by Cailtin Johnstone via Medium.com, ...  \n","1  When they aren’t bashing Bitcoin as a bubble, ...  \n","2  It has been a reasonably subdued weekend of tr...  \n","3  The number of cryptocurrency hedge fund launch...  \n","4  The blockchain business is expected to explode...  "]},"metadata":{"tags":[]},"execution_count":33}]},{"metadata":{"id":"ydsc-4NUN6jA","colab_type":"text"},"cell_type":"markdown","source":["### (1.4) EDA on Cleaned Author Dataframe"]},{"metadata":{"id":"y6vIkwK-N8sa","colab_type":"code","outputId":"a83cd2a8-f371-4a32-9040-af776169ac88","executionInfo":{"status":"ok","timestamp":1554655022608,"user_tz":420,"elapsed":1175,"user":{"displayName":"SHUN LIN","photoUrl":"https://lh4.googleusercontent.com/-pkp40ccE7So/AAAAAAAAAAI/AAAAAAAAAU4/Upp1QcV6fHs/s64/photo.jpg","userId":"16137932526864003348"}},"colab":{"base_uri":"https://localhost:8080/","height":559}},"cell_type":"code","source":["print(\"There are \" + str(len(cleaned_author_df)) + \" entries in this dataframe.\")\n","sum_of_nans = sum(len(cleaned_author_df) - cleaned_author_df.count())\n","print(\"There are \" + str(sum_of_nans) + \" Nan values in the dataframe.\")\n","\n","# take a look at the news scores dataframe\n","cleaned_author_df.head()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["There are 5263 entries in this dataframe.\n","There are 7 Nan values in the dataframe.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>author</th>\n","      <th>contents</th>\n","      <th>description</th>\n","      <th>publisher</th>\n","      <th>source_url</th>\n","      <th>title</th>\n","      <th>date</th>\n","      <th>time</th>\n","      <th>Open</th>\n","      <th>...</th>\n","      <th>Volume_(BTC)</th>\n","      <th>Volume_(Currency)</th>\n","      <th>Weighted_Price</th>\n","      <th>Average</th>\n","      <th>Volatility</th>\n","      <th>SD</th>\n","      <th>publisherLabel</th>\n","      <th>Date_x</th>\n","      <th>Date_y</th>\n","      <th>Mark</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>Michelle Fox</td>\n","      <td>var postloadfunctions var foresee enabled var ...</td>\n","      <td>Bitcoin may still drop to $7,500, but it will ...</td>\n","      <td>CNBC</td>\n","      <td>https://www.cnbc.com/2018/02/01/bitcoin-near-b...</td>\n","      <td>Bitcoin near bottom, will rally to $20,000 thi...</td>\n","      <td>2018-02-02</td>\n","      <td>00:02:00</td>\n","      <td>8547.864403</td>\n","      <td>...</td>\n","      <td>35.615632</td>\n","      <td>300510.207066</td>\n","      <td>8547.594042</td>\n","      <td>12603.493539</td>\n","      <td>433909.334353</td>\n","      <td>2087.647258</td>\n","      <td>146</td>\n","      <td>2018-02-02</td>\n","      <td>2018-02-02</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>3</td>\n","      <td>Bruce Kleinman</td>\n","      <td>demons digital gold part already done please r...</td>\n","      <td>Demons in Digital Gold, Part 5</td>\n","      <td>Hackernoon.com</td>\n","      <td>https://hackernoon.com/remediation-wherefore-a...</td>\n","      <td>Remediation, wherefore art thou?</td>\n","      <td>2018-02-02</td>\n","      <td>00:18:34</td>\n","      <td>8547.864403</td>\n","      <td>...</td>\n","      <td>35.615632</td>\n","      <td>300510.207066</td>\n","      <td>8547.594042</td>\n","      <td>12603.493539</td>\n","      <td>433909.334353</td>\n","      <td>2087.647258</td>\n","      <td>452</td>\n","      <td>2018-02-02</td>\n","      <td>2018-02-02</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>4</td>\n","      <td>Jason Murphy</td>\n","      <td>email password remember feb systems underpinni...</td>\n","      <td>While the systems underpinning bitcoin are tru...</td>\n","      <td>Crikey.com.au</td>\n","      <td>https://www.crikey.com.au/2018/02/02/cryptotra...</td>\n","      <td>Cryptotragedy: what if bitcoin’s greatest stre...</td>\n","      <td>2018-02-02</td>\n","      <td>00:25:09</td>\n","      <td>8547.864403</td>\n","      <td>...</td>\n","      <td>35.615632</td>\n","      <td>300510.207066</td>\n","      <td>8547.594042</td>\n","      <td>12603.493539</td>\n","      <td>433909.334353</td>\n","      <td>2087.647258</td>\n","      <td>220</td>\n","      <td>2018-02-02</td>\n","      <td>2018-02-02</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>5</td>\n","      <td>CoinTelegraph By Molly Jane Zuckerman</td>\n","      <td>cointelegraph jordan belfort wolf wall street ...</td>\n","      <td>Jordan Belfort, the “Wolf of Wall Street”, cal...</td>\n","      <td>Cointelegraph.com</td>\n","      <td>https://cointelegraph.com/news/wolf-of-wall-st...</td>\n","      <td>Wolf Of Wall Street Says Bitcoin Could Hit $50...</td>\n","      <td>2018-02-02</td>\n","      <td>00:49:21</td>\n","      <td>8547.864403</td>\n","      <td>...</td>\n","      <td>35.615632</td>\n","      <td>300510.207066</td>\n","      <td>8547.594042</td>\n","      <td>12603.493539</td>\n","      <td>433909.334353</td>\n","      <td>2087.647258</td>\n","      <td>199</td>\n","      <td>2018-02-02</td>\n","      <td>2018-02-02</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>8</td>\n","      <td>Jonathan Berr</td>\n","      <td>bitcoin dropped friday reflecting plunge almos...</td>\n","      <td>Bitcoin has lost almost 60 percent of its valu...</td>\n","      <td>CBS News</td>\n","      <td>https://www.cbsnews.com/news/cryptocurrency-pr...</td>\n","      <td>Cryptocurrency prices plunge as regulators cla...</td>\n","      <td>2018-02-02</td>\n","      <td>01:42:56</td>\n","      <td>8547.864403</td>\n","      <td>...</td>\n","      <td>35.615632</td>\n","      <td>300510.207066</td>\n","      <td>8547.594042</td>\n","      <td>12603.493539</td>\n","      <td>433909.334353</td>\n","      <td>2087.647258</td>\n","      <td>145</td>\n","      <td>2018-02-02</td>\n","      <td>2018-02-02</td>\n","      <td>1.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 23 columns</p>\n","</div>"],"text/plain":["   Unnamed: 0                                 author  \\\n","0           1                           Michelle Fox   \n","1           3                         Bruce Kleinman   \n","2           4                           Jason Murphy   \n","3           5  CoinTelegraph By Molly Jane Zuckerman   \n","4           8                          Jonathan Berr   \n","\n","                                            contents  \\\n","0  var postloadfunctions var foresee enabled var ...   \n","1  demons digital gold part already done please r...   \n","2  email password remember feb systems underpinni...   \n","3  cointelegraph jordan belfort wolf wall street ...   \n","4  bitcoin dropped friday reflecting plunge almos...   \n","\n","                                         description          publisher  \\\n","0  Bitcoin may still drop to $7,500, but it will ...               CNBC   \n","1                     Demons in Digital Gold, Part 5     Hackernoon.com   \n","2  While the systems underpinning bitcoin are tru...      Crikey.com.au   \n","3  Jordan Belfort, the “Wolf of Wall Street”, cal...  Cointelegraph.com   \n","4  Bitcoin has lost almost 60 percent of its valu...           CBS News   \n","\n","                                          source_url  \\\n","0  https://www.cnbc.com/2018/02/01/bitcoin-near-b...   \n","1  https://hackernoon.com/remediation-wherefore-a...   \n","2  https://www.crikey.com.au/2018/02/02/cryptotra...   \n","3  https://cointelegraph.com/news/wolf-of-wall-st...   \n","4  https://www.cbsnews.com/news/cryptocurrency-pr...   \n","\n","                                               title        date      time  \\\n","0  Bitcoin near bottom, will rally to $20,000 thi...  2018-02-02  00:02:00   \n","1                   Remediation, wherefore art thou?  2018-02-02  00:18:34   \n","2  Cryptotragedy: what if bitcoin’s greatest stre...  2018-02-02  00:25:09   \n","3  Wolf Of Wall Street Says Bitcoin Could Hit $50...  2018-02-02  00:49:21   \n","4  Cryptocurrency prices plunge as regulators cla...  2018-02-02  01:42:56   \n","\n","          Open  ...   Volume_(BTC)  Volume_(Currency)  Weighted_Price  \\\n","0  8547.864403  ...      35.615632      300510.207066     8547.594042   \n","1  8547.864403  ...      35.615632      300510.207066     8547.594042   \n","2  8547.864403  ...      35.615632      300510.207066     8547.594042   \n","3  8547.864403  ...      35.615632      300510.207066     8547.594042   \n","4  8547.864403  ...      35.615632      300510.207066     8547.594042   \n","\n","        Average     Volatility           SD  publisherLabel      Date_x  \\\n","0  12603.493539  433909.334353  2087.647258             146  2018-02-02   \n","1  12603.493539  433909.334353  2087.647258             452  2018-02-02   \n","2  12603.493539  433909.334353  2087.647258             220  2018-02-02   \n","3  12603.493539  433909.334353  2087.647258             199  2018-02-02   \n","4  12603.493539  433909.334353  2087.647258             145  2018-02-02   \n","\n","       Date_y  Mark  \n","0  2018-02-02   1.0  \n","1  2018-02-02   1.0  \n","2  2018-02-02   1.0  \n","3  2018-02-02   1.0  \n","4  2018-02-02   1.0  \n","\n","[5 rows x 23 columns]"]},"metadata":{"tags":[]},"execution_count":34}]},{"metadata":{"id":"Z4nsUEh9OIUV","colab_type":"text"},"cell_type":"markdown","source":["### EDA on One Hot Dataframe"]},{"metadata":{"id":"H3bipu32OLiA","colab_type":"code","outputId":"dd1200a0-9e39-4a27-9298-9a5c6f5d1b34","executionInfo":{"status":"ok","timestamp":1554655023948,"user_tz":420,"elapsed":1395,"user":{"displayName":"SHUN LIN","photoUrl":"https://lh4.googleusercontent.com/-pkp40ccE7So/AAAAAAAAAAI/AAAAAAAAAU4/Upp1QcV6fHs/s64/photo.jpg","userId":"16137932526864003348"}},"colab":{"base_uri":"https://localhost:8080/","height":576}},"cell_type":"code","source":["print(\"There are \" + str(len(onehot_df)) + \" entries in this dataframe.\")\n","sum_of_nans = sum(len(onehot_df) - onehot_df.count())\n","print(\"There are \" + str(sum_of_nans) + \" Nan values in the dataframe.\")\n","\n","# take a look at the news scores dataframe\n","onehot_df.head()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["There are 1592 entries in this dataframe.\n","There are 0 Nan values in the dataframe.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>author</th>\n","      <th>contents</th>\n","      <th>description</th>\n","      <th>publisher</th>\n","      <th>source_url</th>\n","      <th>title</th>\n","      <th>date</th>\n","      <th>time</th>\n","      <th>Open</th>\n","      <th>...</th>\n","      <th>SD</th>\n","      <th>publisherLabel</th>\n","      <th>Mark</th>\n","      <th>publisher_L</th>\n","      <th>author_L</th>\n","      <th>score_sentiment</th>\n","      <th>magnitude_sentiment</th>\n","      <th>tfidf</th>\n","      <th>publisher_L_onehot</th>\n","      <th>author_L_onehot</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>662</td>\n","      <td>Thomas Delahunty</td>\n","      <td>bitcoin news price information analysis almost...</td>\n","      <td>almost ten years since blockchain technology u...</td>\n","      <td>Newsbtc.com</td>\n","      <td>https://www.newsbtc.com/2018/02/05/israel-beco...</td>\n","      <td>Israel Becoming “Crypto Powerhouse,” But ICO R...</td>\n","      <td>2/5/18</td>\n","      <td>0:00:17</td>\n","      <td>7615.109937</td>\n","      <td>...</td>\n","      <td>2146.000443</td>\n","      <td>723.0</td>\n","      <td>-1.0</td>\n","      <td>3.0</td>\n","      <td>2.0</td>\n","      <td>-0.2</td>\n","      <td>0.2</td>\n","      <td>0.29965</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>664</td>\n","      <td>The Star Online</td>\n","      <td>advertisement monday feb across lloyds bank ba...</td>\n","      <td>lloyds banking group plc said sunday would ban...</td>\n","      <td>Thestar.com.my</td>\n","      <td>http://www.thestar.com.my/business/business-ne...</td>\n","      <td>Lloyds Bank to ban credit card owners from buy...</td>\n","      <td>2/5/18</td>\n","      <td>0:26:03</td>\n","      <td>7615.109937</td>\n","      <td>...</td>\n","      <td>2146.000443</td>\n","      <td>1056.0</td>\n","      <td>-1.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.34320</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>665</td>\n","      <td>Mydealz.de</td>\n","      <td>kann man der blase sch n beim platzen zuschaue...</td>\n","      <td>itunes aktuell gibt es die app crypto pro bitc...</td>\n","      <td>Mydealz.de</td>\n","      <td>https://www.mydealz.de/deals/crypto-pro-bitcoi...</td>\n","      <td>Crypto Pro: Bitcoin Ticker kostenlos für iOS</td>\n","      <td>2/5/18</td>\n","      <td>0:39:15</td>\n","      <td>7615.109937</td>\n","      <td>...</td>\n","      <td>2146.000443</td>\n","      <td>695.0</td>\n","      <td>-1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.7</td>\n","      <td>0.7</td>\n","      <td>0.21495</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>667</td>\n","      <td>JTMusic</td>\n","      <td>added feb jtmusic tags someone left messege bi...</td>\n","      <td>bought bitcoin peak k dropping k hype gone sun</td>\n","      <td>Liveleak.com</td>\n","      <td>https://www.liveleak.com/view?i=f9d_1517791670</td>\n","      <td>Someone left me a messege about Bitcoin</td>\n","      <td>2/5/18</td>\n","      <td>0:50:37</td>\n","      <td>7615.109937</td>\n","      <td>...</td>\n","      <td>2146.000443</td>\n","      <td>621.0</td>\n","      <td>-1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.1</td>\n","      <td>0.1</td>\n","      <td>0.65890</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>668</td>\n","      <td>marcuss</td>\n","      <td>one persistent yet baffling narratives frames ...</td>\n","      <td>one persistent yet baffling narratives frames ...</td>\n","      <td>Valuewalk.com</td>\n","      <td>http://www.valuewalk.com/2018/02/cryptocurrenc...</td>\n","      <td>The Worst Argument Against Cryptocurrencies</td>\n","      <td>2/5/18</td>\n","      <td>0:52:53</td>\n","      <td>7615.109937</td>\n","      <td>...</td>\n","      <td>2146.000443</td>\n","      <td>1117.0</td>\n","      <td>-1.0</td>\n","      <td>3.0</td>\n","      <td>0.0</td>\n","      <td>-0.9</td>\n","      <td>0.9</td>\n","      <td>0.35514</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 28 columns</p>\n","</div>"],"text/plain":["   Unnamed: 0            author  \\\n","0         662  Thomas Delahunty   \n","1         664   The Star Online   \n","2         665        Mydealz.de   \n","3         667           JTMusic   \n","4         668           marcuss   \n","\n","                                            contents  \\\n","0  bitcoin news price information analysis almost...   \n","1  advertisement monday feb across lloyds bank ba...   \n","2  kann man der blase sch n beim platzen zuschaue...   \n","3  added feb jtmusic tags someone left messege bi...   \n","4  one persistent yet baffling narratives frames ...   \n","\n","                                         description       publisher  \\\n","0  almost ten years since blockchain technology u...     Newsbtc.com   \n","1  lloyds banking group plc said sunday would ban...  Thestar.com.my   \n","2  itunes aktuell gibt es die app crypto pro bitc...      Mydealz.de   \n","3     bought bitcoin peak k dropping k hype gone sun    Liveleak.com   \n","4  one persistent yet baffling narratives frames ...   Valuewalk.com   \n","\n","                                          source_url  \\\n","0  https://www.newsbtc.com/2018/02/05/israel-beco...   \n","1  http://www.thestar.com.my/business/business-ne...   \n","2  https://www.mydealz.de/deals/crypto-pro-bitcoi...   \n","3     https://www.liveleak.com/view?i=f9d_1517791670   \n","4  http://www.valuewalk.com/2018/02/cryptocurrenc...   \n","\n","                                               title    date     time  \\\n","0  Israel Becoming “Crypto Powerhouse,” But ICO R...  2/5/18  0:00:17   \n","1  Lloyds Bank to ban credit card owners from buy...  2/5/18  0:26:03   \n","2       Crypto Pro: Bitcoin Ticker kostenlos für iOS  2/5/18  0:39:15   \n","3            Someone left me a messege about Bitcoin  2/5/18  0:50:37   \n","4        The Worst Argument Against Cryptocurrencies  2/5/18  0:52:53   \n","\n","          Open       ...                  SD  publisherLabel  Mark  \\\n","0  7615.109937       ...         2146.000443           723.0  -1.0   \n","1  7615.109937       ...         2146.000443          1056.0  -1.0   \n","2  7615.109937       ...         2146.000443           695.0  -1.0   \n","3  7615.109937       ...         2146.000443           621.0  -1.0   \n","4  7615.109937       ...         2146.000443          1117.0  -1.0   \n","\n","   publisher_L  author_L  score_sentiment  magnitude_sentiment    tfidf  \\\n","0          3.0       2.0             -0.2                  0.2  0.29965   \n","1          2.0       2.0              0.0                  0.0  0.34320   \n","2          0.0       0.0              0.7                  0.7  0.21495   \n","3          0.0       0.0              0.1                  0.1  0.65890   \n","4          3.0       0.0             -0.9                  0.9  0.35514   \n","\n","   publisher_L_onehot  author_L_onehot  \n","0                   0                0  \n","1                   0                0  \n","2                   0                0  \n","3                   0                0  \n","4                   0                0  \n","\n","[5 rows x 28 columns]"]},"metadata":{"tags":[]},"execution_count":35}]},{"metadata":{"id":"dEvVjZn3SkXP","colab_type":"text"},"cell_type":"markdown","source":["## (2) Language Models\n","\n","We will try to build a language model for our news score dataset to extract additional information.\n","\n","**NOTE**: We will be trying to build different language models with different architectures. We will also build models using tensorflow and keras for comparsion."]},{"metadata":{"id":"ylPt5Z28Xj5H","colab_type":"text"},"cell_type":"markdown","source":["### (2.1) Preprocessing our data"]},{"metadata":{"id":"3J_QxrpFS2ks","colab_type":"code","colab":{}},"cell_type":"code","source":["# helper methods\n","\n","def numerize_sequence(tokenized):\n","    return [w2i.get(w, unkI) for w in tokenized]\n","def pad_sequence(numerized, pad_index, to_length):\n","    pad = numerized[:to_length]\n","    padded = pad + [pad_index] * (to_length - len(pad))\n","    mask = [w != pad_index for w in padded]\n","    return padded, mask"],"execution_count":0,"outputs":[]},{"metadata":{"id":"mMjEz58vV7P0","colab_type":"code","colab":{}},"cell_type":"code","source":["dataset_df = news_score_df[[\"title\"]]\n","dataset = dataset_df.to_dict('records')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ofsrpPapYMuq","colab_type":"code","outputId":"96fe8951-41ff-45bc-cdea-2a0d56918e2f","executionInfo":{"status":"ok","timestamp":1554922594208,"user_tz":420,"elapsed":1570,"user":{"displayName":"SHUN LIN","photoUrl":"https://lh4.googleusercontent.com/-pkp40ccE7So/AAAAAAAAAAI/AAAAAAAAAU4/Upp1QcV6fHs/s64/photo.jpg","userId":"16137932526864003348"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["input_length = 0\n","for a in dataset:\n","    tokenized_title = tokenizer.word_tokenizer(a['title'].lower())\n","    input_length = max(input_length, len(tokenized_title))\n","    a['tokenized'] = tokenized_title\n","print(input_length)"],"execution_count":9,"outputs":[{"output_type":"stream","text":["50\n"],"name":"stdout"}]},{"metadata":{"id":"iecSKIzrYNYp","colab_type":"code","colab":{}},"cell_type":"code","source":["word_counts = Counter()\n","for a in dataset:\n","    word_counts.update(a['tokenized'])"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Ze5LV5ijZgi3","colab_type":"code","outputId":"69692f93-9edc-419f-af9a-eec950b9cae6","executionInfo":{"status":"ok","timestamp":1554922596105,"user_tz":420,"elapsed":897,"user":{"displayName":"SHUN LIN","photoUrl":"https://lh4.googleusercontent.com/-pkp40ccE7So/AAAAAAAAAAI/AAAAAAAAAU4/Upp1QcV6fHs/s64/photo.jpg","userId":"16137932526864003348"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["# Creating the vocab\n","vocab_size = len(word_counts)\n","special_words = [\"<START>\", \"UNK\", \"PAD\"]\n","vocabulary = special_words + [w for w, c in word_counts.most_common(vocab_size-len(special_words))]\n","w2i = {w: i for i, w in enumerate(vocabulary)}\n","\n","# Numerizing and padding\n","unkI, padI, startI = w2i['UNK'], w2i['PAD'], w2i['<START>']\n","\n","for a in dataset:\n","    a['numerized'] = numerize_sequence(a['tokenized']) # Change words to IDs\n","    a['numerized'], a['mask'] = pad_sequence(a['numerized'], padI, input_length) # Append appropriate PAD tokens\n","    \n","# Compute fraction of words that are UNK:\n","word_counters = Counter([w for a in dataset for w in a['title'] if w != padI])\n","\n","print(\"Fraction of UNK words:\", float(word_counters[unkI]) / sum(word_counters.values()))"],"execution_count":11,"outputs":[{"output_type":"stream","text":["Fraction of UNK words: 0.0\n"],"name":"stdout"}]},{"metadata":{"id":"VrAXaUUYZ0WZ","colab_type":"code","outputId":"0c1ba74d-dd8c-437b-9964-4f930944d5cd","executionInfo":{"status":"ok","timestamp":1554922600427,"user_tz":420,"elapsed":548,"user":{"displayName":"SHUN LIN","photoUrl":"https://lh4.googleusercontent.com/-pkp40ccE7So/AAAAAAAAAAI/AAAAAAAAAU4/Upp1QcV6fHs/s64/photo.jpg","userId":"16137932526864003348"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"cell_type":"code","source":["vocab_size = len(vocabulary)\n","input_length = len(dataset[0]['numerized']) # The length of the first element in the dataset, they are all of the same length\n","\n","d_train, d_valid = train_test_split(dataset, test_size=0.01, random_state=42)\n","\n","print(\"Vocabulary Size:\", vocab_size)\n","print(\"Number of training samples:\",len(d_train))\n","print(\"Number of validation samples:\",len(d_valid))"],"execution_count":12,"outputs":[{"output_type":"stream","text":["Vocabulary Size: 6866\n","Number of training samples: 7029\n","Number of validation samples: 72\n"],"name":"stdout"}]},{"metadata":{"id":"vl-epkKRaGmB","colab_type":"code","outputId":"af7ddb8c-bbf7-4ac9-e265-494c86104ebe","executionInfo":{"status":"ok","timestamp":1554655071961,"user_tz":420,"elapsed":667,"user":{"displayName":"SHUN LIN","photoUrl":"https://lh4.googleusercontent.com/-pkp40ccE7So/AAAAAAAAAAI/AAAAAAAAAU4/Upp1QcV6fHs/s64/photo.jpg","userId":"16137932526864003348"}},"colab":{"base_uri":"https://localhost:8080/","height":71}},"cell_type":"code","source":["def numerized2text(numerized):    \n","    words = [vocabulary[int(num)] for num in numerized]\n","    converted_string = ' '.join(words)\n","    return converted_string\n","\n","entry = d_train[100]\n","print(\"Reversing the numerized: \"+numerized2text(entry['numerized']))\n","print(\"From the `title` entry: \"+ entry['title'])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Reversing the numerized: the sec chairman is ' open ' to the regulation of bitcoin and other cryptocurrencies PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD\n","From the `title` entry: The SEC chairman is 'open' to the regulation of Bitcoin and other cryptocurrencies\n"],"name":"stdout"}]},{"metadata":{"id":"FoCwB-9xadBx","colab_type":"text"},"cell_type":"markdown","source":["### (2.2) Building Model"]},{"metadata":{"id":"P46hY9_PaQ7S","colab_type":"code","colab":{}},"cell_type":"code","source":["def build_batch(dataset, batch_size):\n","    \n","    # randomize the indices we want to get the batch of\n","    indices = list(np.random.randint(0, len(dataset), size=batch_size))\n","    \n","    # indice into the batch\n","    batch = [dataset[i] for i in indices]\n","    \n","    # Get the raw numerized for this input\n","    batch_numerized = np.asarray([db_element[\"numerized\"] for db_element in batch])\n","\n","    # Create an array of start_index that will be concatenated at position 1 for input\n","    start_tokens = np.zeros((batch_size, 1))\n","    batch_input = np.concatenate((start_tokens, batch_numerized), axis=1)\n","\n","    # Remove the last word from each element in the batch to \"shift\" input\n","    batch_input = batch_input[:, :-1]\n","    \n","    # The target should be the un-shifted numerized input\n","    batch_target = batch_numerized\n","\n","    # The target-mask is a 0 or 1 filter to note which tokens are\n","    # padding or not, to give the loss, so the model doesn't get rewarded for\n","    # predicting PAD tokens.\n","    batch_target_mask = np.array([a['mask'] for a in batch])\n","        \n","    return batch_input, batch_target, batch_target_mask"],"execution_count":0,"outputs":[]},{"metadata":{"id":"_ux4FZaeaw8P","colab_type":"code","colab":{}},"cell_type":"code","source":["# Using a basic RNN/LSTM for Language modeling\n","class LanguageModel():\n","    def __init__(self, input_length, vocab_size, rnn_size, learning_rate=1e-4):\n","        self.input_num = tf.placeholder(tf.int32, shape=[None, input_length])\n","        self.targets = tf.placeholder(tf.int32, shape=[None, input_length])\n","        self.targets_mask = tf.placeholder(tf.bool, shape=[None, input_length])\n","        self.embedding = tf.Variable(tf.random_uniform([vocab_size, rnn_size], -1.0, 1.0))\n","        input_emb = tf.nn.embedding_lookup(self.embedding, self.input_num)\n","        lm_cell = tf.nn.rnn_cell.LSTMCell(rnn_size)\n","        outputs, states = tf.nn.dynamic_rnn(lm_cell, input_emb, dtype=tf.float32)\n","        self.output_logits = tf.layers.dense(inputs=outputs, units=vocab_size)\n","        weights = tf.cast(self.targets_mask, tf.float32)\n","        self.loss = tf.losses.sparse_softmax_cross_entropy(labels=self.targets,logits=self.output_logits, weights=weights)\n","        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate, name='Adam')     \n","        self.global_step = tf.train.get_or_create_global_step()\n","        self.train_op = optimizer.minimize(self.loss, global_step=self.global_step)\n","        self.saver = tf.train.Saver()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"48Cy77pXbD1m","colab_type":"text"},"cell_type":"markdown","source":["### (2.3) Create Model"]},{"metadata":{"id":"sCamFzWFa1VB","colab_type":"code","outputId":"abc03c34-a9e4-4187-f1c4-24663fe95a5b","executionInfo":{"status":"ok","timestamp":1554655077570,"user_tz":420,"elapsed":1909,"user":{"displayName":"SHUN LIN","photoUrl":"https://lh4.googleusercontent.com/-pkp40ccE7So/AAAAAAAAAAI/AAAAAAAAAU4/Upp1QcV6fHs/s64/photo.jpg","userId":"16137932526864003348"}},"colab":{"base_uri":"https://localhost:8080/","height":343}},"cell_type":"code","source":["tf.reset_default_graph()\n","model = LanguageModel(input_length=input_length, vocab_size=vocab_size, rnn_size=256, learning_rate=1e-4)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Colocations handled automatically by placer.\n","WARNING:tensorflow:From <ipython-input-45-14bef12bd089>:24: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n","WARNING:tensorflow:From <ipython-input-45-14bef12bd089>:37: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n","WARNING:tensorflow:From <ipython-input-45-14bef12bd089>:43: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use keras.layers.dense instead.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/losses/losses_impl.py:209: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.cast instead.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_grad.py:425: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.cast instead.\n"],"name":"stdout"}]},{"metadata":{"id":"-s8bZMkkbTcU","colab_type":"text"},"cell_type":"markdown","source":["### (2.4) Train Model"]},{"metadata":{"id":"FtfYzgENbWBZ","colab_type":"code","outputId":"1f5a5616-8b4e-4fa1-af04-bdee4f340b63","colab":{"base_uri":"https://localhost:8080/","height":37893}},"cell_type":"code","source":["# DO NOT RUN THIS BLOCK IF YOU DON'T WANT TO TRAIN THE NETWORK\n","\n","experiment = root_folder+\"models/tf_language_model\"\n","plot_info_path = root_folder+\"plots/tf_language_model.csv\"\n","\n","plot_info = pd.DataFrame(columns=['training_err', 'validation_err'])\n","\n","with tf.Session() as sess:\n","    sess.run(tf.global_variables_initializer())\n","    \n","    # Here is how you restore the weights previously saved\n","    # model.saver.restore(sess, experiment)\n","    # also need to restore plot_info from plot_info_path\n","    \n","    epoch = 20000000\n","    batch_size = 64\n","    num_iter = epoch * len(d_train) // batch_size\n","    print(\"Total number of iterations is: \" + str(num_iter))\n","    \n","    eval_input, eval_target, eval_target_mask = build_batch(d_valid, 50)\n","    feed = {model.input_num: eval_input, model.targets: eval_target, model.targets_mask: eval_target_mask}\n","    eval_loss = sess.run(model.loss, feed_dict=feed)\n","    print(\"Evaluation set loss: \", eval_loss)\n","        \n","    for i in range(num_iter):\n","        # Here is how you obtain a batch:\n","        batch_input, batch_target, batch_target_mask = build_batch(d_train, batch_size)\n","        # Map the values to each tensor in a `feed_dict`\n","        feed = {model.input_num: batch_input, model.targets: batch_target, model.targets_mask: batch_target_mask}\n","\n","        # Obtain a single value of the loss for that batch.\n","        # !IMPORTANT! Don't forget to include the train_op to when using a batch from the training dataset\n","        # (d_train)\n","        # !MORE IMPORTANT! Don't use the train_op if you evaluate the loss on the validation set,\n","        # Otherwise, your network will overfit on your validation dataset.\n","\n","        step, train_loss, _ = sess.run([model.global_step, model.loss, model.train_op], feed_dict=feed)\n","\n","        # record info for graphs every 20 steps\n","        if i % 20 == 0 and i % 200 != 0:\n","          eval_input, eval_target, eval_target_mask = build_batch(d_valid, 50)\n","          feed = {model.input_num: eval_input, model.targets: eval_target, model.targets_mask: eval_target_mask}\n","          eval_loss_steps = sess.run(model.loss, feed_dict=feed)\n","          row = {'training_err': train_loss, 'validation_err': eval_loss_steps}\n","          plot_info.loc[len(plot_info)] = row\n","          \n","        # save weights info every 200 steps\n","        if i % 200 == 0:\n","            print(\"step: \" + str(i))\n","            print(\"train_loss: \" + str(train_loss))\n","            eval_input, eval_target, eval_target_mask = build_batch(d_valid, 50)\n","            feed = {model.input_num: eval_input, model.targets: eval_target, model.targets_mask: eval_target_mask}\n","            eval_loss_steps = sess.run(model.loss, feed_dict=feed)\n","            # if (eval_loss_steps < eval_loss):\n","            #  print(\"eval_loss decreases!\")\n","            eval_loss = eval_loss_steps\n","            print(\"Evaluation set loss: \", eval_loss)\n","            print(\"saving plot info so far ....\")\n","            plot_info.to_csv(plot_info_path, index=False)\n","            print(\"saving plot info so far completed ....\")\n","            print(\"saving model weights ....\")\n","            model.saver.save(sess, experiment)\n","            print(\"saving model weights completed ....\")\n","            # else:\n","            #  print(\"eval_loss didn't decrease.\")\n","            #  print(\"half learning rate, make another model, reset to previous checkpoint\")\n","            #  # learning_rate /= 2\n","            #  # model = LanguageModel(input_length=input_length, vocab_size=vocab_size, rnn_size=256*4, learning_rate=learning_rate)\n","            #  model.saver.restore(sess, experiment)\n","    \n","    # Here is how you save the model weights\n","    model.saver.save(sess, experiment)\n","    \n","    # Here is how you restore the weights previously saved\n","    model.saver.restore(sess, experiment)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Total number of iterations is: 2196562500\n","Evaluation set loss:  8.838022\n","step: 0\n","train_loss: 8.836808\n","Evaluation set loss:  8.8373\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 200\n","train_loss: 6.9612474\n","Evaluation set loss:  7.1714764\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 400\n","train_loss: 6.9272366\n","Evaluation set loss:  7.0312114\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 600\n","train_loss: 6.7018175\n","Evaluation set loss:  7.11697\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 800\n","train_loss: 6.790313\n","Evaluation set loss:  6.88539\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 1000\n","train_loss: 6.5747714\n","Evaluation set loss:  6.645661\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 1200\n","train_loss: 6.5367565\n","Evaluation set loss:  6.7653804\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 1400\n","train_loss: 6.623859\n","Evaluation set loss:  6.6837277\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 1600\n","train_loss: 6.337526\n","Evaluation set loss:  6.256702\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 1800\n","train_loss: 6.038297\n","Evaluation set loss:  6.520869\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 2000\n","train_loss: 5.936203\n","Evaluation set loss:  6.4006243\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 2200\n","train_loss: 6.20971\n","Evaluation set loss:  6.474833\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 2400\n","train_loss: 6.2251554\n","Evaluation set loss:  6.227722\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 2600\n","train_loss: 5.9363785\n","Evaluation set loss:  6.0763297\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 2800\n","train_loss: 5.6343327\n","Evaluation set loss:  5.8855104\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 3000\n","train_loss: 5.34447\n","Evaluation set loss:  5.88489\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 3200\n","train_loss: 5.440174\n","Evaluation set loss:  5.9842343\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 3400\n","train_loss: 5.101962\n","Evaluation set loss:  6.004407\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 3600\n","train_loss: 5.3499274\n","Evaluation set loss:  5.6960893\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 3800\n","train_loss: 5.09728\n","Evaluation set loss:  5.6395097\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 4000\n","train_loss: 5.072417\n","Evaluation set loss:  5.596928\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 4200\n","train_loss: 4.8976974\n","Evaluation set loss:  5.5483823\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 4400\n","train_loss: 4.96808\n","Evaluation set loss:  5.5098486\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 4600\n","train_loss: 4.7653284\n","Evaluation set loss:  5.4130483\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 4800\n","train_loss: 4.824798\n","Evaluation set loss:  5.3173966\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 5000\n","train_loss: 4.578667\n","Evaluation set loss:  5.318828\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 5200\n","train_loss: 4.397109\n","Evaluation set loss:  5.3255305\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 5400\n","train_loss: 4.4141035\n","Evaluation set loss:  5.104012\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 5600\n","train_loss: 4.5593834\n","Evaluation set loss:  5.36749\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 5800\n","train_loss: 4.3937817\n","Evaluation set loss:  5.289245\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 6000\n","train_loss: 3.983128\n","Evaluation set loss:  5.358306\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 6200\n","train_loss: 4.3719397\n","Evaluation set loss:  5.150685\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 6400\n","train_loss: 3.9435377\n","Evaluation set loss:  4.730099\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 6600\n","train_loss: 4.193117\n","Evaluation set loss:  4.599087\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 6800\n","train_loss: 3.925778\n","Evaluation set loss:  4.6520505\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 7000\n","train_loss: 3.5902941\n","Evaluation set loss:  5.321266\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 7200\n","train_loss: 3.714779\n","Evaluation set loss:  4.998055\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 7400\n","train_loss: 3.4061873\n","Evaluation set loss:  5.1954546\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 7600\n","train_loss: 3.8286703\n","Evaluation set loss:  4.009703\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 7800\n","train_loss: 3.552699\n","Evaluation set loss:  4.6764064\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 8000\n","train_loss: 3.4309027\n","Evaluation set loss:  4.297708\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 8200\n","train_loss: 3.4357479\n","Evaluation set loss:  4.141822\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 8400\n","train_loss: 3.2192724\n","Evaluation set loss:  4.368823\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 8600\n","train_loss: 3.1368215\n","Evaluation set loss:  4.509535\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 8800\n","train_loss: 3.0909276\n","Evaluation set loss:  4.604878\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 9000\n","train_loss: 3.1988435\n","Evaluation set loss:  3.797773\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 9200\n","train_loss: 2.741303\n","Evaluation set loss:  4.835957\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 9400\n","train_loss: 3.0497081\n","Evaluation set loss:  4.534574\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 9600\n","train_loss: 3.032215\n","Evaluation set loss:  4.081789\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 9800\n","train_loss: 2.8596318\n","Evaluation set loss:  3.7792864\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 10000\n","train_loss: 2.7562726\n","Evaluation set loss:  4.0412307\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 10200\n","train_loss: 2.4948108\n","Evaluation set loss:  4.795138\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 10400\n","train_loss: 2.6450906\n","Evaluation set loss:  3.9080076\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 10600\n","train_loss: 2.566766\n","Evaluation set loss:  4.3879085\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 10800\n","train_loss: 2.6513696\n","Evaluation set loss:  3.8978217\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 11000\n","train_loss: 2.7228715\n","Evaluation set loss:  3.7189527\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 11200\n","train_loss: 2.495302\n","Evaluation set loss:  4.435953\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 11400\n","train_loss: 2.6109502\n","Evaluation set loss:  4.372888\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 11600\n","train_loss: 2.4265096\n","Evaluation set loss:  4.31752\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 11800\n","train_loss: 2.3095782\n","Evaluation set loss:  4.2758684\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 12000\n","train_loss: 2.6320071\n","Evaluation set loss:  3.9474473\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 12200\n","train_loss: 2.393258\n","Evaluation set loss:  3.5342584\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 12400\n","train_loss: 2.352566\n","Evaluation set loss:  4.445291\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 12600\n","train_loss: 2.2629907\n","Evaluation set loss:  3.9644969\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 12800\n","train_loss: 2.1554022\n","Evaluation set loss:  4.9356503\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 13000\n","train_loss: 2.1995492\n","Evaluation set loss:  3.0344877\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 13200\n","train_loss: 2.0770736\n","Evaluation set loss:  4.021648\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 13400\n","train_loss: 1.9594356\n","Evaluation set loss:  3.5965893\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 13600\n","train_loss: 1.8378298\n","Evaluation set loss:  2.4586673\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 13800\n","train_loss: 1.8903044\n","Evaluation set loss:  3.222746\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 14000\n","train_loss: 1.9094609\n","Evaluation set loss:  2.8660047\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 14200\n","train_loss: 1.89417\n","Evaluation set loss:  3.2992477\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 14400\n","train_loss: 1.9396417\n","Evaluation set loss:  4.316798\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 14600\n","train_loss: 1.8708653\n","Evaluation set loss:  3.5312407\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 14800\n","train_loss: 1.660848\n","Evaluation set loss:  4.6135387\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 15000\n","train_loss: 1.6245803\n","Evaluation set loss:  3.8838007\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 15200\n","train_loss: 1.7843065\n","Evaluation set loss:  4.232591\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 15400\n","train_loss: 1.773791\n","Evaluation set loss:  3.2271237\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 15600\n","train_loss: 1.6623834\n","Evaluation set loss:  4.019169\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 15800\n","train_loss: 1.5447463\n","Evaluation set loss:  3.8417892\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 16000\n","train_loss: 1.8031771\n","Evaluation set loss:  3.198356\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 16200\n","train_loss: 1.6659458\n","Evaluation set loss:  3.2130923\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 16400\n","train_loss: 1.5497631\n","Evaluation set loss:  3.667916\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 16600\n","train_loss: 1.4685143\n","Evaluation set loss:  2.6873918\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 16800\n","train_loss: 1.5477142\n","Evaluation set loss:  3.331079\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 17000\n","train_loss: 1.5741154\n","Evaluation set loss:  3.3506682\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 17200\n","train_loss: 1.5501579\n","Evaluation set loss:  2.915423\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 17400\n","train_loss: 1.5242631\n","Evaluation set loss:  2.434775\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 17600\n","train_loss: 1.4843173\n","Evaluation set loss:  3.855279\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 17800\n","train_loss: 1.4797362\n","Evaluation set loss:  3.3291996\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 18000\n","train_loss: 1.3465816\n","Evaluation set loss:  4.000293\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 18200\n","train_loss: 1.3790059\n","Evaluation set loss:  3.544131\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 18400\n","train_loss: 1.4578325\n","Evaluation set loss:  3.3368158\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 18600\n","train_loss: 1.3683828\n","Evaluation set loss:  3.1882086\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 18800\n","train_loss: 1.4636331\n","Evaluation set loss:  4.061835\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 19000\n","train_loss: 1.235378\n","Evaluation set loss:  3.9390688\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 19200\n","train_loss: 1.5426894\n","Evaluation set loss:  3.6789427\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 19400\n","train_loss: 1.3655514\n","Evaluation set loss:  3.236876\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 19600\n","train_loss: 1.176062\n","Evaluation set loss:  2.655898\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 19800\n","train_loss: 1.3091278\n","Evaluation set loss:  3.2437475\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 20000\n","train_loss: 1.1998488\n","Evaluation set loss:  2.6414354\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 20200\n","train_loss: 1.258816\n","Evaluation set loss:  3.713623\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 20400\n","train_loss: 1.1403669\n","Evaluation set loss:  3.7759097\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 20600\n","train_loss: 1.1911566\n","Evaluation set loss:  2.5394888\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 20800\n","train_loss: 1.1912559\n","Evaluation set loss:  3.340204\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 21000\n","train_loss: 1.2996736\n","Evaluation set loss:  3.4138474\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 21200\n","train_loss: 1.1700618\n","Evaluation set loss:  2.4150267\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 21400\n","train_loss: 1.1279141\n","Evaluation set loss:  3.2002492\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 21600\n","train_loss: 1.1186469\n","Evaluation set loss:  2.787463\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 21800\n","train_loss: 1.0363903\n","Evaluation set loss:  3.0149674\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 22000\n","train_loss: 1.207309\n","Evaluation set loss:  3.2863097\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 22200\n","train_loss: 1.1972982\n","Evaluation set loss:  3.4988017\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 22400\n","train_loss: 1.0567564\n","Evaluation set loss:  3.0062556\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 22600\n","train_loss: 1.0806413\n","Evaluation set loss:  4.116283\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 22800\n","train_loss: 1.047705\n","Evaluation set loss:  2.9184413\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 23000\n","train_loss: 1.1170462\n","Evaluation set loss:  3.6316195\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 23200\n","train_loss: 1.0209522\n","Evaluation set loss:  2.2405865\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 23400\n","train_loss: 1.1746392\n","Evaluation set loss:  4.166371\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 23600\n","train_loss: 0.99958646\n","Evaluation set loss:  3.5596578\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 23800\n","train_loss: 1.0559226\n","Evaluation set loss:  3.7955906\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 24000\n","train_loss: 1.0636902\n","Evaluation set loss:  3.3121176\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 24200\n","train_loss: 1.059296\n","Evaluation set loss:  3.0864778\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 24400\n","train_loss: 1.094054\n","Evaluation set loss:  2.5178757\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 24600\n","train_loss: 1.0672868\n","Evaluation set loss:  3.9482985\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 24800\n","train_loss: 1.1167284\n","Evaluation set loss:  3.7376652\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 25000\n","train_loss: 0.9653179\n","Evaluation set loss:  4.723139\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 25200\n","train_loss: 1.0219827\n","Evaluation set loss:  2.590777\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 25400\n","train_loss: 0.90812206\n","Evaluation set loss:  3.6153812\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 25600\n","train_loss: 1.1036636\n","Evaluation set loss:  3.0070894\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 25800\n","train_loss: 1.0379972\n","Evaluation set loss:  3.7791755\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 26000\n","train_loss: 1.0138735\n","Evaluation set loss:  1.7243311\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 26200\n","train_loss: 0.8869505\n","Evaluation set loss:  3.0734913\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 26400\n","train_loss: 1.0027488\n","Evaluation set loss:  2.621781\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 26600\n","train_loss: 1.0497456\n","Evaluation set loss:  4.035085\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 26800\n","train_loss: 0.9594539\n","Evaluation set loss:  2.1180115\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 27000\n","train_loss: 0.9544262\n","Evaluation set loss:  2.8785033\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 27200\n","train_loss: 0.92166436\n","Evaluation set loss:  2.606934\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 27400\n","train_loss: 0.95579416\n","Evaluation set loss:  2.826662\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 27600\n","train_loss: 0.9329781\n","Evaluation set loss:  3.1996832\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 27800\n","train_loss: 0.9918633\n","Evaluation set loss:  3.7776244\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 28000\n","train_loss: 0.89007723\n","Evaluation set loss:  3.1856148\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 28200\n","train_loss: 0.8374092\n","Evaluation set loss:  3.1703825\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 28400\n","train_loss: 0.8726514\n","Evaluation set loss:  3.7847416\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 28600\n","train_loss: 0.93254167\n","Evaluation set loss:  3.2918184\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 28800\n","train_loss: 0.8943223\n","Evaluation set loss:  4.022541\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 29000\n","train_loss: 0.87813085\n","Evaluation set loss:  2.4027965\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 29200\n","train_loss: 0.9423365\n","Evaluation set loss:  3.4763234\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 29400\n","train_loss: 0.8842876\n","Evaluation set loss:  3.90749\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 29600\n","train_loss: 0.8366546\n","Evaluation set loss:  2.1691618\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 29800\n","train_loss: 0.88658845\n","Evaluation set loss:  2.4564812\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 30000\n","train_loss: 0.89314115\n","Evaluation set loss:  3.0386653\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 30200\n","train_loss: 0.92411816\n","Evaluation set loss:  3.2600396\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 30400\n","train_loss: 0.93476963\n","Evaluation set loss:  4.6822057\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 30600\n","train_loss: 0.87226224\n","Evaluation set loss:  3.0943801\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 30800\n","train_loss: 0.9858317\n","Evaluation set loss:  3.7908247\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 31000\n","train_loss: 0.8997914\n","Evaluation set loss:  3.480457\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 31200\n","train_loss: 0.82278585\n","Evaluation set loss:  3.2616065\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 31400\n","train_loss: 0.9076938\n","Evaluation set loss:  2.6014817\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 31600\n","train_loss: 0.8680747\n","Evaluation set loss:  3.940548\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 31800\n","train_loss: 0.81499845\n","Evaluation set loss:  3.5512426\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 32000\n","train_loss: 0.8424416\n","Evaluation set loss:  3.8516867\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 32200\n","train_loss: 0.9176843\n","Evaluation set loss:  3.532965\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 32400\n","train_loss: 0.8289116\n","Evaluation set loss:  3.791118\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 32600\n","train_loss: 0.832143\n","Evaluation set loss:  4.1483107\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 32800\n","train_loss: 0.9291889\n","Evaluation set loss:  4.0395246\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 33000\n","train_loss: 0.89090985\n","Evaluation set loss:  2.1286397\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 33200\n","train_loss: 0.8020601\n","Evaluation set loss:  3.0556839\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 33400\n","train_loss: 0.8574178\n","Evaluation set loss:  3.5195985\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 33600\n","train_loss: 0.87343675\n","Evaluation set loss:  2.686779\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 33800\n","train_loss: 0.8533656\n","Evaluation set loss:  3.8721986\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 34000\n","train_loss: 0.7994813\n","Evaluation set loss:  3.145491\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 34200\n","train_loss: 0.8180189\n","Evaluation set loss:  2.7045276\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 34400\n","train_loss: 0.8507779\n","Evaluation set loss:  3.2287376\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 34600\n","train_loss: 0.7705872\n","Evaluation set loss:  3.8957598\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 34800\n","train_loss: 0.80064416\n","Evaluation set loss:  2.8538935\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 35000\n","train_loss: 0.8872999\n","Evaluation set loss:  2.4518301\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 35200\n","train_loss: 0.8223718\n","Evaluation set loss:  4.1617513\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 35400\n","train_loss: 0.8425739\n","Evaluation set loss:  4.1298933\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 35600\n","train_loss: 0.8658437\n","Evaluation set loss:  3.4014344\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 35800\n","train_loss: 0.7971393\n","Evaluation set loss:  4.388751\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 36000\n","train_loss: 0.88198584\n","Evaluation set loss:  3.479395\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 36200\n","train_loss: 0.82273495\n","Evaluation set loss:  1.9991249\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 36400\n","train_loss: 0.78691345\n","Evaluation set loss:  3.3795044\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 36600\n","train_loss: 0.8064546\n","Evaluation set loss:  2.8614206\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 36800\n","train_loss: 0.8671931\n","Evaluation set loss:  2.7004373\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 37000\n","train_loss: 0.8058435\n","Evaluation set loss:  2.593446\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 37200\n","train_loss: 0.85200065\n","Evaluation set loss:  2.5890865\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 37400\n","train_loss: 0.8863985\n","Evaluation set loss:  3.1446288\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 37600\n","train_loss: 0.87189484\n","Evaluation set loss:  3.7412915\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 37800\n","train_loss: 0.769672\n","Evaluation set loss:  2.9495478\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 38000\n","train_loss: 0.7665797\n","Evaluation set loss:  5.1850133\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 38200\n","train_loss: 0.83826953\n","Evaluation set loss:  3.0308845\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 38400\n","train_loss: 0.8071695\n","Evaluation set loss:  3.7627125\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 38600\n","train_loss: 0.8218163\n","Evaluation set loss:  4.565257\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 38800\n","train_loss: 0.7725137\n","Evaluation set loss:  3.947361\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 39000\n","train_loss: 0.80120784\n","Evaluation set loss:  3.7018497\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 39200\n","train_loss: 0.82065713\n","Evaluation set loss:  2.5125287\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 39400\n","train_loss: 0.8173913\n","Evaluation set loss:  4.305066\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 39600\n","train_loss: 0.82821083\n","Evaluation set loss:  3.3886654\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 39800\n","train_loss: 0.80679655\n","Evaluation set loss:  3.1000235\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 40000\n","train_loss: 0.9003379\n","Evaluation set loss:  3.0062244\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 40200\n","train_loss: 0.8117245\n","Evaluation set loss:  2.4510703\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 40400\n","train_loss: 0.8392847\n","Evaluation set loss:  2.845804\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 40600\n","train_loss: 0.8427397\n","Evaluation set loss:  2.7831032\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 40800\n","train_loss: 0.81855285\n","Evaluation set loss:  3.1126726\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 41000\n","train_loss: 0.8022411\n","Evaluation set loss:  3.5433276\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 41200\n","train_loss: 0.80291325\n","Evaluation set loss:  3.0505066\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 41400\n","train_loss: 0.7863026\n","Evaluation set loss:  3.1832318\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 41600\n","train_loss: 0.80079\n","Evaluation set loss:  3.2048373\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 41800\n","train_loss: 0.7993292\n","Evaluation set loss:  3.3785224\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 42000\n","train_loss: 0.7968523\n","Evaluation set loss:  3.7696602\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 42200\n","train_loss: 0.7863356\n","Evaluation set loss:  3.4644904\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 42400\n","train_loss: 0.7505318\n","Evaluation set loss:  3.2129912\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 42600\n","train_loss: 0.8193903\n","Evaluation set loss:  3.368406\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 42800\n","train_loss: 0.82598376\n","Evaluation set loss:  3.4414322\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 43000\n","train_loss: 0.7312729\n","Evaluation set loss:  4.562586\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 43200\n","train_loss: 0.77317274\n","Evaluation set loss:  2.8514736\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 43400\n","train_loss: 0.7919926\n","Evaluation set loss:  3.5489583\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 43600\n","train_loss: 0.80759716\n","Evaluation set loss:  5.1950974\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 43800\n","train_loss: 0.7689349\n","Evaluation set loss:  4.6748643\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 44000\n","train_loss: 0.8077885\n","Evaluation set loss:  3.74294\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 44200\n","train_loss: 0.7779441\n","Evaluation set loss:  4.6919246\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 44400\n","train_loss: 0.7892523\n","Evaluation set loss:  4.457285\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 44600\n","train_loss: 0.78227705\n","Evaluation set loss:  3.5099788\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 44800\n","train_loss: 0.7753873\n","Evaluation set loss:  2.7672377\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 45000\n","train_loss: 0.79995066\n","Evaluation set loss:  3.8902798\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 45200\n","train_loss: 0.76174676\n","Evaluation set loss:  3.8240771\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 45400\n","train_loss: 0.72687393\n","Evaluation set loss:  3.3656957\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 45600\n","train_loss: 0.79752713\n","Evaluation set loss:  3.468428\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 45800\n","train_loss: 0.77943575\n","Evaluation set loss:  3.2249372\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 46000\n","train_loss: 0.7206068\n","Evaluation set loss:  3.5345871\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 46200\n","train_loss: 0.78851885\n","Evaluation set loss:  3.6882775\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 46400\n","train_loss: 0.74784493\n","Evaluation set loss:  2.571165\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 46600\n","train_loss: 0.7447838\n","Evaluation set loss:  2.5866518\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 46800\n","train_loss: 0.79023355\n","Evaluation set loss:  3.6571186\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 47000\n","train_loss: 0.87915283\n","Evaluation set loss:  3.185864\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 47200\n","train_loss: 0.8010816\n","Evaluation set loss:  3.5197601\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 47400\n","train_loss: 0.75324243\n","Evaluation set loss:  3.9386995\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 47600\n","train_loss: 0.8554858\n","Evaluation set loss:  1.7503979\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 47800\n","train_loss: 0.7795023\n","Evaluation set loss:  2.8486407\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 48000\n","train_loss: 0.72937036\n","Evaluation set loss:  2.327709\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 48200\n","train_loss: 0.71695316\n","Evaluation set loss:  5.1789346\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 48400\n","train_loss: 0.78156954\n","Evaluation set loss:  3.3475509\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 48600\n","train_loss: 0.80199325\n","Evaluation set loss:  2.8733056\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 48800\n","train_loss: 0.82000554\n","Evaluation set loss:  4.369675\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 49000\n","train_loss: 0.7546942\n","Evaluation set loss:  4.8539615\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 49200\n","train_loss: 0.8450171\n","Evaluation set loss:  2.780206\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 49400\n","train_loss: 0.80377233\n","Evaluation set loss:  5.0629\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 49600\n","train_loss: 0.79727334\n","Evaluation set loss:  1.9886101\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 49800\n","train_loss: 0.747552\n","Evaluation set loss:  3.7359648\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 50000\n","train_loss: 0.71550524\n","Evaluation set loss:  3.566058\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 50200\n","train_loss: 0.7968897\n","Evaluation set loss:  3.184225\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 50400\n","train_loss: 0.8446051\n","Evaluation set loss:  3.1896887\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 50600\n","train_loss: 0.7073934\n","Evaluation set loss:  3.3180437\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 50800\n","train_loss: 0.7739857\n","Evaluation set loss:  1.8864913\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 51000\n","train_loss: 0.741242\n","Evaluation set loss:  4.6409335\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 51200\n","train_loss: 0.85064155\n","Evaluation set loss:  2.6211085\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 51400\n","train_loss: 0.7611406\n","Evaluation set loss:  4.191062\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 51600\n","train_loss: 0.83211595\n","Evaluation set loss:  4.5997133\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 51800\n","train_loss: 0.8035023\n","Evaluation set loss:  3.5793598\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 52000\n","train_loss: 0.7253315\n","Evaluation set loss:  4.5857897\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 52200\n","train_loss: 0.76382744\n","Evaluation set loss:  4.2589135\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 52400\n","train_loss: 0.75469595\n","Evaluation set loss:  3.9645727\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 52600\n","train_loss: 0.82372004\n","Evaluation set loss:  4.821385\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 52800\n","train_loss: 0.7485796\n","Evaluation set loss:  3.569342\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 53000\n","train_loss: 0.81180334\n","Evaluation set loss:  5.101831\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 53200\n","train_loss: 0.76182276\n","Evaluation set loss:  2.4958963\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 53400\n","train_loss: 0.72793764\n","Evaluation set loss:  3.1627185\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 53600\n","train_loss: 0.7784219\n","Evaluation set loss:  4.1605597\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 53800\n","train_loss: 0.78315103\n","Evaluation set loss:  4.0561543\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 54000\n","train_loss: 0.7741703\n","Evaluation set loss:  3.5327725\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 54200\n","train_loss: 0.7989238\n","Evaluation set loss:  2.3762798\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 54400\n","train_loss: 0.8130779\n","Evaluation set loss:  3.3116493\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 54600\n","train_loss: 0.80798095\n","Evaluation set loss:  4.188884\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 54800\n","train_loss: 0.7347829\n","Evaluation set loss:  4.651537\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 55000\n","train_loss: 0.71769994\n","Evaluation set loss:  2.6735709\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 55200\n","train_loss: 0.75592524\n","Evaluation set loss:  4.3922014\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 55400\n","train_loss: 0.7638349\n","Evaluation set loss:  3.536651\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 55600\n","train_loss: 0.7438724\n","Evaluation set loss:  4.231634\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 55800\n","train_loss: 0.8182861\n","Evaluation set loss:  5.6153536\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 56000\n","train_loss: 0.8335882\n","Evaluation set loss:  2.5106435\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 56200\n","train_loss: 0.8163793\n","Evaluation set loss:  2.667952\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 56400\n","train_loss: 0.80600554\n","Evaluation set loss:  4.2321587\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 56600\n","train_loss: 0.73715293\n","Evaluation set loss:  4.269575\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 56800\n","train_loss: 0.75298935\n","Evaluation set loss:  2.3945887\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 57000\n","train_loss: 0.7852723\n","Evaluation set loss:  3.2587504\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 57200\n","train_loss: 0.79581136\n","Evaluation set loss:  3.892951\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 57400\n","train_loss: 0.78849816\n","Evaluation set loss:  3.0359836\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 57600\n","train_loss: 0.68140274\n","Evaluation set loss:  3.0178928\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 57800\n","train_loss: 0.7312271\n","Evaluation set loss:  3.7475915\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 58000\n","train_loss: 0.7188093\n","Evaluation set loss:  3.5469625\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 58200\n","train_loss: 0.7622399\n","Evaluation set loss:  2.0896435\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 58400\n","train_loss: 0.7848801\n","Evaluation set loss:  2.991229\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 58600\n","train_loss: 0.77736217\n","Evaluation set loss:  4.0697327\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 58800\n","train_loss: 0.725702\n","Evaluation set loss:  3.944915\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 59000\n","train_loss: 0.7845826\n","Evaluation set loss:  4.1161113\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 59200\n","train_loss: 0.7913497\n","Evaluation set loss:  4.984391\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 59400\n","train_loss: 0.7651079\n","Evaluation set loss:  3.0841327\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 59600\n","train_loss: 0.7183073\n","Evaluation set loss:  3.09591\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 59800\n","train_loss: 0.7543732\n","Evaluation set loss:  2.9703264\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 60000\n","train_loss: 0.7013187\n","Evaluation set loss:  5.3672576\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 60200\n","train_loss: 0.7651949\n","Evaluation set loss:  2.894723\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 60400\n","train_loss: 0.7884225\n","Evaluation set loss:  1.954269\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 60600\n","train_loss: 0.72991234\n","Evaluation set loss:  3.9634235\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 60800\n","train_loss: 0.8178135\n","Evaluation set loss:  5.278192\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 61000\n","train_loss: 0.66162485\n","Evaluation set loss:  2.6151936\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 61200\n","train_loss: 0.7351761\n","Evaluation set loss:  3.6706264\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 61400\n","train_loss: 0.72069985\n","Evaluation set loss:  3.6288762\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 61600\n","train_loss: 0.8154801\n","Evaluation set loss:  4.051816\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 61800\n","train_loss: 0.74190414\n","Evaluation set loss:  2.8609874\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 62000\n","train_loss: 0.7663278\n","Evaluation set loss:  3.623835\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 62200\n","train_loss: 0.74963576\n","Evaluation set loss:  2.5276248\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 62400\n","train_loss: 0.70172095\n","Evaluation set loss:  4.419943\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 62600\n","train_loss: 0.7363854\n","Evaluation set loss:  4.162647\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 62800\n","train_loss: 0.7829881\n","Evaluation set loss:  3.4054322\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 63000\n","train_loss: 0.7347146\n","Evaluation set loss:  3.4818575\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 63200\n","train_loss: 0.74281913\n","Evaluation set loss:  3.2514102\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n","step: 63400\n","train_loss: 0.7658166\n","Evaluation set loss:  3.6493762\n","saving plot info so far ....\n","saving plot info so far completed ....\n","saving model weights ....\n","saving model weights completed ....\n"],"name":"stdout"}]},{"metadata":{"id":"0cf96KxecEGV","colab_type":"text"},"cell_type":"markdown","source":["### (2.4) Evaluate Models\n","\n","We will use different methods to evaluate our models\n","\n","0. Plot Training and Validation error over epochs\n","1.   How our model generate new titles\n","2.   How our model detects unlikely titles\n","3.   How the embedding perform on our data\n","\n"]},{"metadata":{"id":"epGLYb4xiZcQ","colab_type":"text"},"cell_type":"markdown","source":["#### (2.4.0) Plots\n"]},{"metadata":{"id":"f3ymyrZMijK4","colab_type":"code","outputId":"1c5406d6-7caf-435d-b882-cebedbff5efd","executionInfo":{"status":"ok","timestamp":1554655087104,"user_tz":420,"elapsed":836,"user":{"displayName":"SHUN LIN","photoUrl":"https://lh4.googleusercontent.com/-pkp40ccE7So/AAAAAAAAAAI/AAAAAAAAAU4/Upp1QcV6fHs/s64/photo.jpg","userId":"16137932526864003348"}},"colab":{"base_uri":"https://localhost:8080/","height":410}},"cell_type":"code","source":["plot_info_path = root_folder+\"plots/tf_language_model.csv\"\n","plot_df = pd.read_csv(plot_info_path)\n","plot_df_per_epoch = plot_df.groupby(np.arange(len(plot_df))//50).mean()\n","train_error_per_epoch = plot_df_per_epoch['training_err']\n","validation_error_per_epoch = plot_df_per_epoch['validation_err']\n","print(len(plot_df))\n","f, ax = plt.subplots()\n","ax.plot(train_error_per_epoch, 'o-',c='r')\n","ax.plot(validation_error_per_epoch, 'o-',c='g')\n","# Plot legend and use the best location automatically: loc = 0.\n","ax.legend(['Train loss', 'Validation loss'], loc = 0)\n","ax.set_title('Training/Validation loss per Epoch')\n","ax.set_xlabel('Epoch')\n","ax.set_ylabel('Error') \n","plt.plot()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["2853\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["[]"]},"metadata":{"tags":[]},"execution_count":47},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAecAAAFnCAYAAACcvYGMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niM2oOYuIiNiMmrOIiIjNqDmLiIjYzP8BWw37ZyMiNjoAAAAASUVORK5CYII=\n","text/plain":["<Figure size 576x396 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"metadata":{"id":"z6QP9iHRcNlM","colab_type":"text"},"cell_type":"markdown","source":["#### (2.4.1) Generating New Titles"]},{"metadata":{"id":"_wKS9khlbopZ","colab_type":"code","outputId":"6cfcc8c3-57ed-4bd2-9a77-715fa7a0ebb0","executionInfo":{"status":"ok","timestamp":1554655095594,"user_tz":420,"elapsed":7084,"user":{"displayName":"SHUN LIN","photoUrl":"https://lh4.googleusercontent.com/-pkp40ccE7So/AAAAAAAAAAI/AAAAAAAAAU4/Upp1QcV6fHs/s64/photo.jpg","userId":"16137932526864003348"}},"colab":{"base_uri":"https://localhost:8080/","height":258}},"cell_type":"code","source":["model_file = root_folder+\"models/tf_language_model\"\n","\n","with tf.Session() as sess:\n","    model.saver.restore(sess, model_file)\n","\n","    # Here are some headline starters.\n","    # They're all about tech companies, because\n","    # That is what is in our dataset\n","    headline_starters = [\"bitcoin price\", \"today\", \"big\"]\n","    \n","    for headline_starter in headline_starters:\n","        print(\"===================\")\n","        print(\"Generating titles starting with: \"+headline_starter)\n","        \n","        tokenized = tokenizer.word_tokenizer(headline_starter)\n","        current_build = [startI] + numerize_sequence(tokenized)\n","\n","        while len(current_build) < input_length:\n","            current_padded = current_build[:input_length] + [padI] * (input_length - len(current_build))\n","            current_padded = np.array([current_padded])\n","\n","            feed = {model.input_num: current_padded}\n","            logits = sess.run(model.output_logits, feed_dict=feed)\n","\n","            last_index = len(current_build) - 1\n","            last_logits = logits[0][last_index]\n","            \n","            current_build.append(np.argmax(last_logits))\n","        \n","        produced_sentence = numerized2text(current_build)\n","        print(produced_sentence)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use standard file APIs to check for files with this prefix.\n","INFO:tensorflow:Restoring parameters from /content/gdrive/My Drive/Paradigm/Paradigm (Spr 19) - Team 2/Colab-code/models/tf_language_model\n","===================\n","Generating titles starting with: bitcoin price\n","<START> bitcoin price technical analysis for 03 / 15 / 2018 – longer-term downside targets drop for the rout in $ 8,000 ? crypto market cap in common ? $ 5922 month market cap gox and us gox maneuvers in 45 minutes minutes minutes lee lee lee lee levels crushed\n","===================\n","Generating titles starting with: today\n","<START> today : bitcoin is crashing , but bottom still have in the world ' to be worth what it ? ! financial analyst ' in just an online threat to remake the island to this week in by storm , ’ below us $ 6000 , is ' off\n","===================\n","Generating titles starting with: big\n","<START> big banks ban bitcoin purchases using credit cards - report for first time report - coindesk report - coindesk report ; s report ; s more than report illegal activities first report - report report ; report rise more than report illegal | finance chief : cbdt ) ban\n"],"name":"stdout"}]},{"metadata":{"id":"TBnVgDvEc9cY","colab_type":"text"},"cell_type":"markdown","source":["#### (2.4.2) Fake/Unlikely News Headline Detection\n","\n","Lower loss means the headline is more likely."]},{"metadata":{"id":"NViFkWxoc3AJ","colab_type":"code","outputId":"9989d921-f92a-42a2-cdb8-c28b1324f42a","executionInfo":{"status":"ok","timestamp":1554655095913,"user_tz":420,"elapsed":5213,"user":{"displayName":"SHUN LIN","photoUrl":"https://lh4.googleusercontent.com/-pkp40ccE7So/AAAAAAAAAAI/AAAAAAAAAU4/Upp1QcV6fHs/s64/photo.jpg","userId":"16137932526864003348"}},"colab":{"base_uri":"https://localhost:8080/","height":207}},"cell_type":"code","source":["headline1 = \"Bitcoin price crashes\"\n","headline2 = \"Bitcoin price reach a new high\"\n","headline3 = \"bitcoin is legal in all countries\"\n","\n","headlines = [headline1, headline2, headline3]\n","\n","with tf.Session() as sess:\n","    model.saver.restore(sess, model_file)\n","\n","    for headline in headlines:\n","        headline = headline.lower()\n","\n","        tokenized = tokenizer.word_tokenizer(headline)\n","        numerized = numerize_sequence(tokenized)\n","        unkI, padI, startI = w2i['UNK'], w2i['PAD'], w2i['<START>']\n","        \n","        padded, mask = pad_sequence(numerized, padI, input_length)\n","        \n","        hl_element = {}\n","        hl_element['tokenized'] = tokenized\n","        hl_element['numerized'] = padded\n","        hl_element['mask'] = mask\n","        d_hl = [hl_element]\n","        hl_input, hl_target, hl_target_mask = build_batch(d_hl, 1)\n","        feed = {model.input_num: hl_input, model.targets: hl_target, model.targets_mask: hl_target_mask}\n","        loss = sess.run([model.loss], feed_dict=feed)\n","        print(\"----------------------------------------\")\n","        print(\"Headline:\",headline)\n","        print(\"Loss of the headline:\", loss)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["INFO:tensorflow:Restoring parameters from /content/gdrive/My Drive/Paradigm/Paradigm (Spr 19) - Team 2/Colab-code/models/tf_language_model\n","----------------------------------------\n","Headline: bitcoin price crashes\n","Loss of the headline: [6.74614]\n","----------------------------------------\n","Headline: bitcoin price reach a new high\n","Loss of the headline: [8.830537]\n","----------------------------------------\n","Headline: bitcoin is legal in all countries\n","Loss of the headline: [9.069546]\n"],"name":"stdout"}]},{"metadata":{"id":"heAfajU4e_ii","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}