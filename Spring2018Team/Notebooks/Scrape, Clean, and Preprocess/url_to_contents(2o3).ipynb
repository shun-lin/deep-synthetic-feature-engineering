{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import Packages and Libraries ##\n",
    "# make it run on py2 and py3\n",
    "from __future__ import division, print_function\n",
    "# Web parcing, scraping, etc.\n",
    "import bs4 as bs # BeautifulSoup4 \n",
    "import urllib3\n",
    "import re\n",
    "import requests # HTTP parser\n",
    "import html5lib\n",
    "\n",
    "# DataFrames and math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Output related packages \n",
    "import pprint as pp\n",
    "\n",
    "# read-in and write-out\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# stretch Jupyter coding blocks to fit screen\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\")) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Mining II\n",
    "This  notebook is intended to perform the following processes:\n",
    "\n",
    "    2.1 Read-in batched csv files and perform content extraction.\n",
    "\n",
    "    2.2 The notebook uses beautifulsoup to extract paragraph content of each url in batch.\n",
    "\n",
    "    2.3 Content extraction is written out as a matching csv file for future concatenation/merging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### **Begin Data Mining II:** Per-url, article content extraction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__2.1 Read-in batched csv files and perform content extraction.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'rawData5.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-4b5cb21847ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'rawData5.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/data-x/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    676\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/data-x/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/data-x/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    785\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    788\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/data-x/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1015\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/data-x/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1706\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'usecols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1708\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1710\u001b[0m         \u001b[0mpassed_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnames\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File b'rawData5.csv' does not exist"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('rawData5.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__2.2 Use beautifulsoup to extract paragraph content of each url in batch.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__List of known path errors -- for validate_site -- is above the function call for batch_control__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_site(site, ignore):\n",
    "    \"\"\"\n",
    "    accepts single url, and a list. Iterates of list for paths to ignore.\n",
    "    If flag is found in list of know issues, url is flagged for removal.\n",
    "    Returns a flag -- '' -- for 'falsy' check \n",
    "    \"\"\"\n",
    "    from urllib.parse import urlparse\n",
    "    import copy\n",
    "    url = copy.copy(site) \n",
    "    \n",
    "    # Note: urlparse requires a string argument\n",
    "    info = urlparse(str(url))\n",
    "\n",
    "    for rm in ignore:\n",
    "        if rm in info.path:  # validates that site is not a podcast\n",
    "            return('')       # flagged for removal\n",
    "        else:\n",
    "            continue\n",
    "    return(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_content(site):\n",
    "    \"\"\"\n",
    "    Accepts dictionary of single key containing url. \n",
    "    Function uses beautiful soup to parse through paragraphs on given url\n",
    "    Returns the extracted data from paragraphs.\n",
    "    \"\"\"\n",
    "    \n",
    "    src = requests.get(site).content            # accesses content of html object\n",
    "    soup = bs.BeautifulSoup(src, 'lxml')        # object creation used in extracting paragraphs using built-in html parser\n",
    "    body = soup.find_all('p')                   # finds all paragraphs '<p>' in html object\n",
    "    return(body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_from_content(body):\n",
    "    \"\"\"\n",
    "    Iterates over website content extracted by get_content \n",
    "    Function then splits the body into individual sentences and appends then to a list.\n",
    "    Returns said list\n",
    "    \"\"\"\n",
    "    \n",
    "    sentence = [parags.text for parags in body]\n",
    "    text = \"\\t\".join(sentence)                  # tab delimeter for easier extraction\n",
    "    return(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article(site):\n",
    "    \"\"\"\n",
    "    Accepts dict of 1 element containg url. \n",
    "    Function then uses other functions to validate the url -- checks for know issues and discards them.\n",
    "    Function, if no error is detected then calls function to extract article contents\n",
    "    Returns article contents\n",
    "    \"\"\"\n",
    "    \n",
    "    valid_site = validate_site(site, ignore)\n",
    "    if not valid_site:\n",
    "        return('Access Limit Met')                  # uses 'falsy' check if string is empty \n",
    "    else:\n",
    "        body = get_content(valid_site)\n",
    "        text = extract_from_content(body)\n",
    "        if not text:                                # uses 'falsy' check if string is empty \n",
    "            return('403 Forbidden')\n",
    "    return(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text(df):\n",
    "    \"\"\"\n",
    "    Accepts a 6xn dataframe and returns a 1xn list and a modifed dataframe\n",
    "    Note: stitching a dataframes to a list is much faster that updating existing one with a new column\n",
    "    \"\"\"\n",
    "    \n",
    "    import copy\n",
    "    content = []\n",
    "    url = []\n",
    "    author = copy.copy(df['author'])\n",
    "    link = copy.copy(df['source_url'])\n",
    "    \n",
    "    if author == 'Ml-implode.com': \n",
    "        r = requests.get(link)\n",
    "        soup = bs.BeautifulSoup(r.text, 'html.parser')\n",
    "        source = soup.find(id=\"LIJIT_title\")                   # manually found -- may differ moving forward\n",
    "        link = source.find('a').get('href')                    # gets the 'href inside the a tag -- i.e. the correct url\n",
    "        url.append(link)\n",
    "        content.append(get_article(link))                      # send correct url out for extraction\n",
    "    else:\n",
    "        content.append(get_article(link))\n",
    "        \n",
    "    return({'url':url, 'content':content})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_df(df1, df2):\n",
    "    \"\"\"\n",
    "    Accepts a dataframe, and calls a function that checks for known aggregator, and known error sites. \n",
    "    If found, function replaces invalid entries with corrected ones, or drops them.\n",
    "    Returns corrected df.\n",
    "    \"\"\"\n",
    "    \n",
    "    import copy\n",
    "    author = copy.deepcopy(df1['author'])\n",
    "    df = copy.deepcopy(df1)\n",
    "    df['contents'] = \"\".join(df2['content']) # converts list to string\n",
    "    \n",
    "    #checks for know aggregator, replaces it with the url referenced by aggregator\n",
    "    if author == 'Ml-implode.com':\n",
    "        df['source_url'] = \"\".join(df2['url']) # converts 'list' item to 'str' item\n",
    "        return(df)        \n",
    "    else:\n",
    "        return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_author_name(df):\n",
    "    \"\"\"\n",
    "    Accepts dictionary of single url.\n",
    "    It then parses through url to extracts and returns netloc of url  \n",
    "    \"\"\"\n",
    "    \n",
    "    from urllib.parse import urlparse\n",
    "    import copy\n",
    "    url = copy.copy(df) \n",
    "    \n",
    "    # urlparse requires a string \n",
    "    info = urlparse(str(url))\n",
    "    return(info.netloc)          #.netloc extract the main url -- i.e. excludes path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 'None' values\n",
    "def rm_false_author(df,start):\n",
    "    \"\"\"\n",
    "     Receives single row of pandas datraframe, and removes aggregator name from author feature -- if found\n",
    "     Returns original dataframe if no flag found\n",
    "    \"\"\"\n",
    "    \n",
    "    author = df['author']\n",
    "    source = df['source_url']\n",
    "    publisher = df['publisher']\n",
    "    \n",
    "    #checks for know aggregator, replaces it with the url referenced by aggregator\n",
    "    for i in range(len(df)):\n",
    "        if author.loc[i] == 'Ml-implode.com':\n",
    "            author.loc[start] = replace_author_name(source.loc[i])  # parameter is correct url as type 'str'\n",
    "            publisher.loc[start] = author.loc[i]                    # replaces instances of incorrect publisher name\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__List of known publishers and aggregators -- for stitch_df -- is a cell above the function call for batch_control__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stitch_df(df, start, halt, init, discard):\n",
    "    \"\"\"\n",
    "    Accepts a dataframe, start/stop condition, as well as an initializer for proper indexing and a list of row-keys to drop\n",
    "    Controls all other associated functions related to data extraction\n",
    "    Return a preprocessed dataframe\n",
    "    \"\"\"\n",
    "    \n",
    "    import copy\n",
    "    df = copy.deepcopy(df)\n",
    "    data = pd.DataFrame()\n",
    "    start += init\n",
    "    halt += init\n",
    "    terminate = len(df) + init\n",
    "    author = df['author']\n",
    "    \n",
    "    # displays progress\n",
    "    if halt != terminate:\n",
    "        print(\"Batching range:\", start+1,\"-\", halt) \n",
    "    else:\n",
    "        print(\"Batching range:\", start+1,\"-\", terminate)\n",
    "        \n",
    "    # conditional merges dataframes and calls other functions that provide dataframe with data\n",
    "    while start < halt:\n",
    "        \n",
    "        ## print statement comes in handy for debugging ##\n",
    "        #print('\\n',author.loc[start], df['source_url'].loc[start])\n",
    "        \n",
    "        ### Checks for known bogus publisher or aggregator, disposes of row if error causing ###\n",
    "        for rm in discard:\n",
    "            while author.loc[start] == rm:\n",
    "                df.drop([start], axis = 0, inplace = True)\n",
    "                start += 1\n",
    "        ### -------------------------------------------------------------------------------- ###\n",
    "        \n",
    "        # datafram copy to prevent mutability \n",
    "        df1 = copy.deepcopy(df.loc[start])\n",
    "        df2 = get_text(df1)\n",
    "\n",
    "        datum = data.append(combine_df(df1,df2), ignore_index = True)  # append without dropping entries with same row number\n",
    "\n",
    "        ## minor clean up ##\n",
    "        # optional: data = data.drop(columns=['description'])          # removes redundant column        \n",
    "        data = rm_false_author(datum, start)                           # replaces invalid author entry\n",
    "\n",
    "        start += 1\n",
    "    return(data)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Batch Control and out-to-csv fail safe__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_control(df, batch, init, discard):\n",
    "    \"\"\"\n",
    "    Accepts a pandas df, and a batch size, and iterates through extraction and writting as batches.\n",
    "    The process is structured as such, to minimize computation time, and detect errors.\n",
    "    Does not have a Return. Used as master control.\n",
    "    \"\"\"\n",
    "    \n",
    "    start = 0\n",
    "    terminate = len(df)\n",
    "    print(\"Number of articles to be extracted:\",terminate)\n",
    "    print(\"Total number of batches:\", int(np.ceil(terminate/batch)),\"\\n\")\n",
    "    \n",
    "    while start < terminate:\n",
    "        \n",
    "        halt = start + batch   # Batch control\n",
    "    \n",
    "         # ensures we don't over expand range \n",
    "        if halt > terminate:        \n",
    "            halt = start + terminate\n",
    "            \n",
    "        temp = df[start:halt]\n",
    "        temp_out = stitch_df(temp, start, halt, init, discard)\n",
    "        \n",
    "        ### Creates/updates csv file to ensure process batches are not lost due to bugs or errors ###\n",
    "        try:\n",
    "            with open('riskEx_df5.csv') as file:\n",
    "                print('\\t Updating existing csv file')\n",
    "                temp_in = pd.read_csv('riskEx_df5.csv')\n",
    "                temp_out = temp_in.append(temp_out, ignore_index=True)\n",
    "                temp_out.to_csv('riskEx_df5.csv', index_label = False)\n",
    "                print('\\t Updated batch saved to csv')\n",
    "                pass\n",
    "        except IOError as e:\n",
    "            print(\"\\t Creating initial csv file\")\n",
    "            temp_out.to_csv('riskEx_df5.csv', index_label = False)\n",
    "            print('\\t Initial batch saved to csv')\n",
    "        ### ------------------------------------------------------------------------------------- ###\n",
    "        \n",
    "        start += batch         # Batch increment\n",
    "        \n",
    "    print(\"\\n*******************\\n PROJECT COMPLETED\\n*******************\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__2.3 Write out content as csv file__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "---\n",
    "### __NOTE__ \n",
    "__In order to ensure there is zero data override, update the initializer to the same value as the begining of the dataframe slice prior to running batch_control.__ \n",
    "\n",
    "Failing to do so will at best throw an error, and at worst it will override your data. \n",
    "___\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### KEY: initialize 'init' and the first row value in dataframe being passed #####\n",
    "#  Failiing to initialize 'project' and 'init' to same value may lead to data loss #\n",
    "\n",
    "print(\"\\nProcessing\",len(df),\"articles in DataFrame.\")\n",
    "batch_size = 100\n",
    "lhs = 2400\n",
    "rhs = 7000\n",
    "end = len(df)\n",
    "\n",
    "\n",
    "# Initialize project and init\n",
    "init = lhs \n",
    "project = df[lhs:]      # Approximate time for every 1000 articles:  ~25:00 \n",
    "##### ------------------------------------------------------------------------ #####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Lists of common error causing paths and publishers__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of error causing paths\n",
    "ignore = ['mp3', 'rlslog', 'mo4ch', 'rex-silentium-bitcoin-suppressor', 'PD209.html',\n",
    "          'booking.com_considers_adding_flights_rules_out_bitcoin_acceptance', 'PD207.html',\n",
    "         'VL201.html', 'www.wikitimes.net', '1016879623', '.mp4', '1016330206', 'photos',\n",
    "         'General-Tech', 'energy', '5a78bbe5e4b00f94fe93fd8d', '4885392.htm', '.ece', '166075/',\n",
    "         'www.thetruthaboutcars.com', 'www.techishaky.com', 'www.anroed.com', 'dogecoin-wallet.html',\n",
    "         '4884023.htm', 'node/4249884', 'Visconti.html', 'a20180410PD210.html', 'whatthehack.life', \n",
    "         'ac7d222e4b07a3485e49e16', 'a20180330PD206.html', 'a20180326PD205.html', 'forty-seven',\n",
    "         'no-news-today', '1021452297']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Known bogus publishers, and aggregators. \n",
    "#List tells stitch_df to dispose of content where the author or url is one of the following\n",
    "\n",
    "discard = ['PA', 'https://holdernews.com', 'Blogspotpoint.com', 'Connor Madsen', \n",
    "           'Pressreleasepoint.com', 'Pewinternet.org', 'www.pewinternet.org', \n",
    "           'Jeremy Hellstrom', 'Ben Potter', 'Steph Willems', 'Thetruthaboutcars.com', \n",
    "           'Adam Tonge', 'lookout', 'noreply@blogger.com', 'Dan Tan']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Extract Content__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_control(project, batch_size, init, discard)  # execute batch extraction and write out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Finally:__ The following few cells carry out some minor preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual check \n",
    "riskEx = pd.read_csv('riskEx_df5.csv')\n",
    "print(riskEx.info())\n",
    "print(len(riskEx['description'].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Remove rows flagged for removal during preprossesing__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop duplicates, keep first instance of source-url\n",
    "riskEx.drop_duplicates(['description'], keep='first', inplace = True)\n",
    "print(len(riskEx['source_url'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list of labels previously flagged for removal\n",
    "rm_403 = riskEx[riskEx['contents'] == '403 Forbidden'].index.values.tolist()\n",
    "rm_acc_denied = riskEx[riskEx['contents'] == 'Access Limit Met'].index.values.tolist()\n",
    "\n",
    "remove_rows = sorted(rm_403 + rm_forbidden + rm_acc_denied) # list of all values to be removed sorted for iteration\n",
    "print(\"Total unique rows after preprocessing:\", len(riskEx)-len(remove_rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use remove_rows to remove observation from dataframe\n",
    "use_riskEx = riskEx.drop(remove_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_riskEx.info()\n",
    "print(len(use_riskEx['source_url'].unique()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Writting to disk__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## RUN AFTER ENTIRE DF IS COMPLETELY EXTRACTED ##\n",
    "# saves dataframe as a preprocessed and cleaned (slightly) DataFrame\n",
    "use_riskEx.to_csv('use_riskEx5.csv', index_label = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__confirming everything was performed as expected__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv('use_riskEx5.csv')\n",
    "print(df2.info())\n",
    "print(len(df2['description'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df2['timeStamp'].head(3))\n",
    "print(df2['timeStamp'].tail(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Completed Process: \n",
    "##### riskEx_df dataframes are dataframes that contained extracted data but with no preprocessing performed.\n",
    "    They include more information, but there may be repeated values, or urls that are inaccessible\n",
    "    \n",
    "##### use_riskEx dataframes represent various dataframes of extracted features, their values, and the text content of each article's body.\n",
    "    The files have undergone preprocessing and duplicates have been removed. Also gone, unfortunatelly, are the descriptions of rows with urls that were not accessible using BS4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **End Data Mining II:** Per-url, article content extraction\n",
    "___"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
